{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to the VAIC_23_24 Software System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The VAIC software is divided into two parts: the Python software running on the Jetson Nano (directory: JetsonExample) and the C++ software running on the VEX Brain (directory: V5Example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Explanation of Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will provide a brief explanation of some AI concepts that appear in the VAIC program.<br>\n",
    "\n",
    "### 1.Training and Inference of Deep Neural Networks:\n",
    "\n",
    "Deep Neural Networks (DNNs) are artificial neural networks composed of multiple layers (hence \"deep\") of neurons, used to learn complex patterns and functions from large amounts of data. The training and inference of DNNs are two crucial processes that enable the network to perform various tasks such as image recognition, speech recognition, and natural language processing. Here is a brief explanation of these two processes:\n",
    "\n",
    "#### Training Process<br>\n",
    "Training is the process through which a deep neural network learns to map input data to output results. It involves the following steps:\n",
    "\n",
    "Data Preparation: First, training data must be collected and preprocessed, including standardization, normalization, or other forms of data cleaning and preparation to make the data suitable for neural network processing.<br><br>\n",
    "Forward Propagation: The input data is passed forward through the network layers, with each layer applying an activation function (such as ReLU, Sigmoid, etc.) until it reaches the output layer.<br><br>\n",
    "Loss Calculation: At the output layer, the loss value is calculated based on the network's output and the actual expected output (labels). The loss function (such as cross-entropy or mean squared error) measures the difference between the predicted values and the true values.<br><br>\n",
    "Backward Propagation: Using gradient descent (or other optimization algorithms), the network's weights are adjusted to minimize the loss function. This involves calculating the gradient of the loss function with respect to each weight and updating the weights based on these gradients.<br><br>\n",
    "Iterative Optimization: The above forward and backward propagation processes are repeated over multiple epochs, each time using different batches of data from the training set, until the network's performance reaches a satisfactory level or a preset number of iterations is reached. <br><br>\n",
    "Forward propagation is illustrated in Figure 1, and backward propagation is shown in Figure 2.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/前向过程.jpg\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Forward Propagation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Inference Process\n",
    "Inference is the process of using a trained model to make predictions on new data. This typically occurs after training is complete, and the network is deployed for practical applications. The inference process involves the following steps:\n",
    "\n",
    "Data Preprocessing: Similar to the training phase, the input data for inference needs to be appropriately preprocessed.\n",
    "Forward Propagation: The preprocessed data is fed into the trained network for a single forward propagation computation, without the need for backpropagation or weight updates.\n",
    "Output Interpretation: The network's output is interpreted and transformed according to the specific application. For example, in a classification task, the category with the highest probability is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/反向过程.jpg\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Backward Propagation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training is the process of building a model, which requires substantial computational resources and data, while inference is the process of using these models to make predictions, typically demanding high speed and efficiency. Optimizing these two processes is a crucial aspect of deep learning research and application. Below, we will introduce how NVIDIA TensorRT acceleration software is used to optimize the inference process in VAIC.\n",
    "\n",
    "Currently, the VAIC competition is still in its early stages, and the neural network training required for the competition has been completed and provided by VEX AI. As AI technology becomes more widespread and the level of VEX AI competitions improves, it may become possible and necessary for competition teams to train their own neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Introduction to Deep Neural Network Development Platforms:\n",
    "Facebook's PyTorch and Google's TensorFlow are currently the most popular deep learning frameworks, both of which are open-source projects. They support the Python language and provide a wealth of APIs and tools that facilitate the development and training of various deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/pytorch1.jpg\" width=\"500\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch is an open-source Python machine learning library developed by Facebook AI Research (FAIR) based on Torch, designed to provide a flexible deep learning framework.\n",
    "\n",
    "Core Features:\n",
    "Two Core Functionalities: PyTorch offers powerful GPU-accelerated tensor computation (similar to NumPy) and deep neural networks with an automatic differentiation system.\n",
    "Main Characteristics:\n",
    "Dynamic Computation Graph: PyTorch uses a dynamic computation graph, allowing the construction and modification of the computation graph at runtime, which makes the experimentation and debugging process more flexible.\n",
    "Flexibility: PyTorch provides a high degree of flexibility, enabling users to easily define, train, and debug models, and freely customize their models and training processes.\n",
    "Ease of Use: PyTorch's API is simple and intuitive, making it easy to learn and use, allowing developers to quickly get started with deep learning tasks.\n",
    "GPU Acceleration: PyTorch can leverage GPUs to accelerate the training of deep learning models.\n",
    "Applications:\n",
    "PyTorch is widely used in fields such as computer vision, natural language processing, and reinforcement learning. It offers many pre-trained models and tool libraries, such as torchvision, torchtext, and gym, making it easier for developers to build and debug models.\n",
    "Environment Support:\n",
    "PyTorch is compatible with multiple operating systems and platforms, including Windows (CUDA, CPU), macOS (CPU), and Linux (CUDA, ROCm, CPU).\n",
    "PyTorch's simplicity, efficiency, and ease of use make it a crucial tool in the field of deep learning, favored by many developers and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/tensorflow1.jpg\" width=\"500\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TensorFlow is an open-source machine learning framework developed and maintained by Google Brain, Google's AI team. Below is a concise introduction to TensorFlow:\n",
    "\n",
    "Main Characteristics:\n",
    "High Flexibility: Supports various machine learning and deep learning models, including neural networks, deep learning models, reinforcement learning, and more.<br>\n",
    "Cross-Platform: Capable of running on various servers, PC terminals, mobile devices, and cloud servers.<br>\n",
    "High-Performance Computing: Achieves high-performance computation and training through GPU and TPU acceleration.<br>\n",
    "Automatic Differentiation: Provides automatic differentiation functionality, simplifying the model training process.<br>\n",
    "Distributed Computing: Supports distributed computing across multiple machines, enabling large-scale data and model training.\n",
    "Visualization Tools: Offers TensorBoard, a tool for visualizing the model training process and results.<br>\n",
    "Community Support: Has a large community providing extensive documentation, tutorials, and example code, making it easy for users to learn and use.<br>\n",
    "Environment Support:\n",
    "Python: TensorFlow offers a CPU version (tensorflow) and a GPU-accelerated version (tensorflow-gpu), as well as their nightly build versions (tf-nightly, tf-nightly-gpu).<br>\n",
    "Applications:\n",
    "TensorFlow is widely used in machine learning, deep learning, natural language processing, computer vision, reinforcement learning, time series analysis, recommendation systems, and speech recognition.<br>\n",
    "In summary, TensorFlow is a powerful, flexible, cross-platform, high-performance machine learning framework, extensively used in deep learning research and applications across various fields.\n",
    "\n",
    "Note: (tf-nightly, tf-nightly-gpu) are nightly build versions of TensorFlow, providing developers with a preview and opportunity to test the latest features of TensorFlow. As they may contain unstable changes, caution is required when using them, and experimentation and testing should be conducted in an appropriate environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. YOLOv3:\n",
    "As previously introduced, YOLOv3 is a deep convolutional neural network composed of a total of 106 layers. These include various types of layers such as convolutional layers, pooling layers, normalization layers, and skip connections.\n",
    "\n",
    "Its main network structure, known as Darknet-53, is an open-source neural network framework containing 53 convolutional layers. Darknet-53 is most famously used to support the YOLO series of object detection models and serves as the foundation for YOLOv3.\n",
    "\n",
    "Darknet-53 is a powerful feature extractor that includes multiple residual connections. These residual connections help prevent training difficulties in very deep networks. Residual networks aid in optimizing the gradient flow by allowing the input to connect directly to subsequent layers, making it feasible to train deeper networks.\n",
    "\n",
    "In addition to Darknet-53, YOLOv3 incorporates multi-scale prediction and upsampling layers, enabling the network to detect objects at three different scales, thus improving the detection capability for small-sized objects. These additional layers bring the total number of layers in the network to 106. This deep network structure allows YOLOv3 to excel in speed while maintaining high accuracy in detecting objects of various sizes.\n",
    "\n",
    "When trained on the COCO dataset, the YOLOv3 model can recognize 80 different categories, including people, vehicles, animals, etc. This is because the COCO (Common Objects in Context) dataset defines 80 common object categories.\n",
    "\n",
    "It should be noted that the YOLOv3 model used in VAIC has been fine-tuned based on the COCO dataset to adapt to the specific scenarios of the VEX-5 Robotics Competition, such as the competition arena, robot structures, motion components, control hardware, software development platforms, and sensors.\n",
    "\n",
    "The YOLOv3 model used in VAIC has been enhanced to improve the recognition of common objects in the VEX-5 Robotics Competition, such as balls, cubes, and tri-colored balls, as well as color recognition (e.g., red, blue, green objects). For instance, the YOLOv3 used in the VAIC_23_24 season only needs to recognize three types of tri-colored balls. The label.txt file in the directory only contains the labels for the three tri-colored balls: GreenTriball, RedTriball, and BlueTriball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/banner-yolov3.png\" width=\"500\" height=\"150\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. GPU (Graphics Processing Unit):\n",
    "A GPU is a specialized electronic computing device initially designed to accelerate the creation of images to facilitate quick output to display devices. With advancements in technology, the function of GPUs has expanded from mere graphic rendering to complex computational tasks, particularly those requiring parallel processing for AI tasks.\n",
    "\n",
    "Main Characteristics and Uses:\n",
    "Parallel Processing Capability:\n",
    "GPUs possess hundreds to thousands of cores, enabling them to process multiple computational tasks simultaneously. This makes them excel in handling tasks requiring extensive parallel computations, such as graphic rendering, video editing, scientific computing, and machine learning.\n",
    "\n",
    "Graphic Rendering:\n",
    "The primary and most fundamental function of a GPU is to accelerate the graphic rendering process. It can swiftly process complex graphic and image processing algorithms, delivering high-quality video game and 3D animation effects.\n",
    "\n",
    "Scientific Computing and Machine Learning:\n",
    "With increasing computational demands, GPUs have become indispensable tools in scientific research, especially in fields such as physical simulations, climate modeling, and bioinformatics. In machine learning and deep learning, GPUs can significantly accelerate the model training process, particularly when handling large-scale datasets.\n",
    "\n",
    "Accelerating General-Purpose Computing:\n",
    "The high parallel processing capability of GPUs is also utilized to accelerate a wide range of general-purpose computing tasks. Technologies like CUDA (Compute Unified Device Architecture) and OpenCL (Open Computing Language) allow developers to leverage GPUs for non-graphic related computational tasks.\n",
    "\n",
    "NVIDIA's GPUs have a broad impact on high-performance computing and gaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/NVIDIA.jpeg\" width=\"500\" height=\"200\" alt=\"英伟达Jetson Nano\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. TensorRT (Tensor RealTime NVIDIA Inference Engine):\n",
    "NVIDIA's TensorRT is a high-performance deep learning inference engine that can significantly enhance the performance and efficiency of deep learning applications running on NVIDIA GPUs.\n",
    "\n",
    "Main Features:\n",
    "Optimization:\n",
    "TensorRT optimizes deep learning model inference performance through techniques such as layer and tensor fusion, precision calibration (including FP16 and INT8 support), and automatic kernel tuning.\n",
    "\n",
    "Multi-Framework Support:\n",
    "It supports various mainstream deep learning frameworks, such as TensorFlow, PyTorch, and Caffe, allowing models trained with these frameworks to be directly converted into TensorRT-optimized models.\n",
    "\n",
    "Efficient Performance:\n",
    "TensorRT can greatly improve model execution speed and reduce latency, making it especially suitable for real-time processing applications like autonomous driving and robotic navigation.\n",
    "\n",
    "Dynamic Tensor Support:\n",
    "It supports dynamic input, allowing input dimensions to be changed as needed at runtime, providing greater flexibility for various applications.\n",
    "\n",
    "Large-Scale Deployment:\n",
    "TensorRT supports running on different NVIDIA hardware, including servers and edge devices, facilitating large-scale model deployment.\n",
    "\n",
    "Due to its general inference mechanism, TensorRT can directly use pre-trained models for high-speed inference without the need for training. This makes it particularly suitable for edge computing devices like the Jetson Nano, which have relatively limited hardware resources and cannot install large software packages like TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. ONNX (Open Neural Network Exchange):\n",
    "ONNX (Open Neural Network Exchange) is an open format exchange protocol used for converting the parameters of deep learning models. This format enables models to be transferred between different machine learning and deep learning frameworks, facilitating model sharing and deployment.\n",
    "\n",
    "ONNX was jointly initiated by Facebook and Microsoft in 2017 and has gained support from several major tech companies, including Amazon, Google, and IBM.\n",
    "\n",
    "Main Characteristics:\n",
    "Framework Interoperability:\n",
    "ONNX aims to allow developers to train models in one framework (such as TensorFlow, PyTorch, etc.) and easily convert them to other frameworks for inference or further development. This way, developers can leverage the specific advantages of each framework, such as training in one and deploying in another.\n",
    "\n",
    "Standardized Format:\n",
    "ONNX defines a set of standard operations and computations, as well as a standard file format, for describing various common layers and operations in neural networks. This standardization helps simplify model exchange between different platforms.\n",
    "\n",
    "Applications:\n",
    "Model Conversion:\n",
    "Developers can use specialized conversion tools (such as ONNX Converter) to convert models from a specific framework to the ONNX format, and then to other frameworks.\n",
    "\n",
    "Model Deployment:\n",
    "ONNX models can be deployed on various devices and platforms that support ONNX, including servers, mobile devices, and embedded devices.\n",
    "\n",
    "Model Optimization:\n",
    "ONNX provides model optimization tools that can simplify and accelerate models, improving inference efficiency.\n",
    "\n",
    "How to Use ONNX:\n",
    "Train Models:\n",
    "Train models in any framework that supports ONNX (e.g., PyTorch, TensorFlow, etc.).\n",
    "\n",
    "Convert Models:\n",
    "Use the appropriate conversion tools to convert models to the ONNX format.\n",
    "\n",
    "Deploy Models:\n",
    "Deploy models on inference engines or frameworks that support ONNX for inference.\n",
    "\n",
    "In summary, ONNX, as an open and cross-platform neural network model representation format, greatly enhances the portability and accessibility of deep learning models. It allows researchers and developers to more flexibly choose tools and platforms, accelerating the process from research to application.\n",
    "\n",
    "Note the file suffix \"VEXOverUnder.onnx\" in this directory, indicating that it is an ONNX model file. This is actually a fine-tuned YOLOv3 network in ONNX format. In the subsequent program analysis, we will explain how it is converted to a format suitable for TensorRT inference.RT推理的格式。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7 Fine-tuning:\n",
    "Fine-tuning refers to the process of retraining a pre-trained model, which has already been trained on a large-scale dataset, using a specific dataset for the target task to adjust the model's parameters. This enables the model to better adapt to and solve the target task.\n",
    "\n",
    "Utilizing Pre-trained Models:\n",
    "Fine-tuning is usually based on a model that has already been pre-trained on a large-scale dataset, such as ResNet or VGG on ImageNet. These pre-trained models have already learned rich feature representations, providing a good starting point.\n",
    "\n",
    "Adapting to Specific Tasks:\n",
    "By fine-tuning on a specific dataset for the target task, the model can learn task-specific features, enabling it to better adapt to and solve the target task.\n",
    "\n",
    "Adjusting Model Parameters:\n",
    "During the fine-tuning process, the model's parameters are adjusted based on the target task's dataset to optimize the model's performance on the target task.\n",
    "\n",
    "Fine-tuning Steps:\n",
    "Select a Pre-trained Model:\n",
    "Choose an appropriate pre-trained model as a starting point based on the nature of the target task and the characteristics of the dataset.\n",
    "\n",
    "Prepare the Dataset:\n",
    "Prepare the training set, validation set, and test set for the target task, and perform necessary data preprocessing and augmentation.\n",
    "\n",
    "Load the Pre-trained Model:\n",
    "Load the selected pre-trained model and determine the number of layers to fine-tune. Typically, you can choose to fine-tune the entire model or only part of the layers.\n",
    "\n",
    "Modify Model Structure (if needed):\n",
    "Make necessary modifications to the structure of the pre-trained model based on the requirements of the target task, such as changing the number of neurons in the output layer.\n",
    "\n",
    "Set Training Parameters:\n",
    "Set hyperparameters for the training process, such as learning rate, batch size, and number of training epochs.\n",
    "\n",
    "Fine-tuning:\n",
    "Fine-tune the model using the training set of the target task, updating the model's parameters through backpropagation and gradient descent algorithms.\n",
    "\n",
    "Validation and Testing:\n",
    "Evaluate the fine-tuned model using the validation set and test set to verify the model's performance on the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Overview of the VAIC_23_24 Program (Jetson's Python Program and Brain's C++ Program)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jetson's Python Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The reasons why Python has become the preferred language for artificial intelligence (AI) programming mainly include:\n",
    "\n",
    "Simple and Readable Syntax: Its simple and readable syntax makes learning and using it easy.\n",
    "Rich Third-Party Libraries: It has a wealth of third-party libraries such as Scikit-learn, Pandas, and NumPy, as well as deep neural network development platforms like TensorFlow and PyTorch that support complex AI projects.\n",
    "Open Source and Free: Its open-source and free nature lowers the cost of use.\n",
    "Active Community: An active community provides extensive support for developers.\n",
    "Additionally, Python's cross-platform capabilities and powerful data visualization tools (like matplotlib and OpenCV) greatly enhance its applicability in rapid prototyping. These factors collectively drive Python's widespread use in the AI field.\n",
    "\n",
    "C++ compiled programs are well known to the vast user base of VEX 5, so they need not be elaborated here. A simple comparison of their differences:\n",
    "\n",
    "Main Disadvantages of Python Compared to C++:\n",
    "Performance: Python typically runs slower than C++.\n",
    "Memory Efficiency: Python consumes more memory, and its memory control is not as fine-grained as C++.\n",
    "Concurrent Processing: Python's Global Interpreter Lock (GIL) limits its multi-threading performance, whereas C++ offers stronger concurrency support.\n",
    "Real-Time Applications: Python is not suitable for applications requiring high real-time performance.\n",
    "System Access: C++ provides deeper system-level access capabilities.\n",
    "In summary, for applications requiring high performance and complex system control, C++ is a more suitable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/python1.jpg\" width=\"500\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Composition of the Jetson AI Program (Python)\n",
    "Open the VAIC software package and enter the JetsonExample directory. You will see the following Python files, along with a brief explanation of their functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/mulu.jpg\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation of Jetson Python Files:<br>\n",
    "1.common.py: A program provided by NVIDIA for high-speed inference using GPU (CUDA) during video object recognition on Jetson.<br>\n",
    "2.data_processing.py: Also provided by NVIDIA, this program preprocesses images captured by the camera, extracts the shape, color, and position information of target objects through the YOLOv3 neural network, and performs post-processing on this information.<br>\n",
    "3.filter.py: A program for filtering and smoothing real-time GPS data.<br>\n",
    "4.model.py: A program for transforming and loading the YOLOv3 model and performing object recognition inference.<br>\n",
    "5.overunder.py: The main program controlling the robot's movements. It is designed to run automatically, starting the program when the robot system is powered on. It detects the camera, Wi-Fi, VEX GPS, and VEX Brain connections, loads YOLOv3, receives images from the camera, performs inference through YOLOv3, recognizes scene objects, post-processes the recognized object information, and sends the object position and distance information to the Brain. The Brain then selects the target object based on competition planning and uses the position and distance information to control the robot's movements.<br>\n",
    "6.V5Comm.py: The communication program that packages and sends the detected target object position and depth information from Jetson to the VEX Brain.<br>\n",
    "7.V5MapPosition.py: A program that calculates the position of the target object relative to the robot's rotation center and locates the robot's position on the field using VEX GPS coordinates.<br>\n",
    "8.V5Position.py: A program that receives and processes the position coordinate data detected by the VEX GPS sent by the Brain.<br>\n",
    "9.V5Web.py: A program for remote visualization of the robot's movement status and camera images through the web.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other Auxiliary Files:\n",
    "camera_offsets.json: Contains the coordinate offsets of the camera installation position relative to the robot's rotation center, the units of length used, and the angular offsets with respect to the longitudinal and lateral axes. This is used to calibrate the camera for accurate object coordinate and distance calculations. These data need to be measured and recorded by the competitors based on the actual camera installation.\n",
    "\n",
    "gps_offsets.json: Contains the coordinate offsets of the VEX GPS installation position relative to the robot's rotation center, the units of length used, and the angular offset with respect to the longitudinal axis. This is used to calibrate the coordinates and distance of recognized objects on the competition field. These data need to be measured and recorded by the competitors based on the actual GPS installation.\n",
    "\n",
    "labels.txt: Labels for classifying the tri-colored balls.\n",
    "\n",
    "VEXOverUnder.onnx: The ONNX format model of YOLOv3, used in the program to be converted to NVIDIA's .engine format for high-speed inference with TensorRT.\n",
    "\n",
    "README.md: A MarkDown file providing a brief description of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Brain's C++ Program\n",
    "Open the VAIC software package and enter the V5Example directory. In the ai_demo subdirectory, you can see C++ files under the src folder and header files under the include folder. Here is a brief explanation of their functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/V5目录.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The aidemo directory contains all the control programs for the Brain side, composed of the following .cpp and .h files:\n",
    "\n",
    "ai_jetson.cpp and ai_jetson.h: Define functions related to communication with the Jetson.<br>\n",
    "ai_function.cpp and ai_function.h: Define functions for controlling the robot's movements based on the target categories and position coordinates recognized by Jetson's vision system.<br>\n",
    "ai_robot_link.cpp and ai_robot_link.h: Define communication functions for coordinating the movements of two robots during the autonomous interaction competition phase.<br>\n",
    "dashboard.cpp and dashboard.h: Define functions for displaying dynamic interaction data from Jetson and ai_robot_link on the Brain's screen.<br>\n",
    "main.cpp: The main control program and entry point of the application.<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
