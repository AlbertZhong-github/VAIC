{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c023a785-90dc-407c-a8b0-d43a887d01cf",
   "metadata": {},
   "source": [
    "\n",
    "## Detailed Explanation of the VAIC_23_24 Program\n",
    "\n",
    "This article provides a detailed explanation of the establishment, inference, object recognition data processing of the YOLOv3 visual neural network by the VAIC_23_24 program. It also delves into the working mechanism of the Intel D435 camera, communication between Jetson Nano and Vex Brian, and how neural network recognition of object positions and distances is used to control robot motion via Brian. Additionally, it covers mutual communication between dual machines to achieve coordinated movement, as well as remote real-time display technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42797586",
   "metadata": {},
   "source": [
    "\n",
    "The AI functionality of the VAIC_23_24 software is implemented by the Python programs in the JetsonExample directory.<br> The components of the Jetson Python programs have been briefly introduced earlier. Please refer to the \"Introduction to VAIC Software\".<br> \n",
    "As an AI project, we will first conduct a technical analysis of this part to help readers understand and grasp the application and implementation of AI functionality in the program as soon as possible.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d1735",
   "metadata": {},
   "source": [
    "###   1.The establishment of VAIC visual neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0d33f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The AI technology introduced by Vex AI uses a visual depth neural network to recognize objects and measure distances in real-time images captured by the camera on the field. The shape, color, and positional data of the objects are then sent to Vex Brian. The Brian robot uses this information to select targets, plan paths, and control movement and actions based on the competition strategy.\n",
    "\n",
    "This section provides an in-depth discussion on the integration of the visual depth neural network YOLOv3, the reception of images from the Intel D435 camera, the use of TensorRT for high-speed inference to identify object positions and distances, and the process of transmitting the identified object data to Brian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d605bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Open \"overunder.py,\" and you can see a series of instances created for the program's execution within the__def init(self)__ function of the \"MainApp\" class:\n",
    "\n",
    "Our discussion starts from the \"Processing\" class.\n",
    "\n",
    "The AI process begins with the creation of an instance of \"Processing.\" According to Python's execution mechanism, after allocating memory space for the instance, the__def init(self)__ function of \"Processing\" is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.processing = Processing(self.camera.depth_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19e061",
   "metadata": {},
   "source": [
    "The__init__(self) code segment is as follows:\n",
    "\n",
    "The parameter depth_scale is a scaling factor used in depth calculations by the camera. The initialization of the camera is configured to obtain settings from the camera, which will be explained in the Camera section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e947ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Class to handle camera data processing, preparing for inference, and running inference\n",
    "    # on camera image.\n",
    "    def __init__(self, depth_scale):\n",
    "        self.depth_scale = depth_scale\n",
    "        self.align_to = rs.stream.color\n",
    "        self.align = rs.align(self.align_to)  # Align depth frames to color stream\n",
    "        self.model = Model()  # Initialize the object detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0af09",
   "metadata": {},
   "source": [
    ".self.depth_scale = depth_scale<br>\n",
    "This line stores the depth calculation scaling factor from the external parameter into the internal parameter for use in calculating the distance to recognized objects.\n",
    "\n",
    ".self.align_to = rs.stream.color<br>\n",
    ".self.align = rs.align(self.align_to)<br>\n",
    "Image alignment: In RealSense applications, depth and object color images are collected separately by infrared and visible light cameras. The depth image needs to be aligned with the RGB image so that both types of image data can be processed in the same coordinate system.\n",
    "\n",
    "Next, an instance of the Model class is created:\n",
    "\n",
    ".self.model = Model()<br>\n",
    "The Model class is the core of VAIC, representing the neural network model for depth measurement and object recognition, and is a crucial part of VAIC's application of AI technology. <br>\n",
    "It is important to note that the definition of class Model is in the model.py file. The overunder.py program imports this definition at the beginning with the statement:<br>\n",
    "from model import Model<br>\n",
    "Now, let's take a look at the__init__(self) function of the Model instance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        # Initialize the TensorRT engine and execution context.\n",
    "\n",
    "        # Define file paths for ONNX and engine files\n",
    "        current_folder_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        onnx_file_path = os.path.join(current_folder_path, \"VEXOverUnder.onnx\")\n",
    "        # If you change the onnx file to your own model, adjust the file name here\n",
    "        engine_file_path = os.path.join(current_folder_path, \"VEXOverUnder.trt\")\n",
    "        # This should match the .onnx file name\n",
    "\n",
    "        # Get the TensorRT engine\n",
    "        self.engine = Model.get_engine(onnx_file_path, engine_file_path)\n",
    "\n",
    "        # Create an execution context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        # Allocate buffers for input and output\n",
    "        self.inputs, self.outputs, self.bindings, self.stream =\n",
    "                                                common.allocate_buffers(self.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87565909",
   "metadata": {},
   "source": [
    "\n",
    "Based on the previous introduction, to use NVIDIA TensorRT for high-speed inference, the neural network needs to be optimized and converted to a different format, changing the .onnx file format to .trt format.<br> \n",
    "The first three lines of the code check whether the .onnx and .trt files exist.<br>\n",
    " The .onnx file here is the YOLOv3 neural network fine-tuned using PyTorch by VEX AI, specifically for the recognition of tri-colored balls and their distances for the VAIC_23_24 season.<br>\n",
    "\n",
    "The line self.engine = Model.get_engine(onnx_file_path, engine_file_path) is the format conversion function. The function is as follows:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb16ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "       def build_engine():\n",
    "            print(\"Building engine file from onnx, this could take a while\")\n",
    "            # Builds and returns a TensorRT engine from an ONNX file.\n",
    "            with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "                    builder.create_network(common.EXPLICIT_BATCH) as network, \\\n",
    "                    builder.create_builder_config() as config, \\\n",
    "                    trt.OnnxParser(network, TRT_LOGGER) as parser, \\\n",
    "                    trt.Runtime(TRT_LOGGER) as runtime:\n",
    "\n",
    "                config.max_workspace_size = 1 << 28  # Set maximum workspace size to 256MiB\n",
    "                builder.max_batch_size = 1\n",
    "\n",
    "                # Check if ONNX file exists\n",
    "                if not os.path.exists(onnx_file_path):\n",
    "                    print(\"ONNX file {} not found.\".format(onnx_file_path))\n",
    "                    exit(0)\n",
    "\n",
    "                # Load and parse the ONNX file\n",
    "                with open(onnx_file_path, \"rb\") as model:\n",
    "                    if not parser.parse(model.read()):\n",
    "                        print(\"ERROR: Failed to parse the ONNX file.\")\n",
    "                        for error in range(parser.num_errors):\n",
    "                            print(parser.get_error(error))\n",
    "                        return None\n",
    "\n",
    "                # Set input shape for the network\n",
    "                network.get_input(0).shape = [1, 3, 320, 320]\n",
    "\n",
    "                # Build and serialize the network, then create and return the engine\n",
    "                plan = builder.build_serialized_network(network, config)\n",
    "                engine = runtime.deserialize_cuda_engine(plan)\n",
    "                with open(engine_file_path, \"wb\") as f:\n",
    "                    f.write(plan)\n",
    "                return engine\n",
    "\n",
    "        # Check if a serialized engine file exists and load it if so, otherwise build a new\n",
    "        # one\n",
    "        if os.path.exists(engine_file_path):\n",
    "            with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "                return runtime.deserialize_cuda_engine(f.read())\n",
    "        else:\n",
    "            return build_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb7bdc",
   "metadata": {},
   "source": [
    "Following Python's alignment rules, the program executes from the bottom up. If the .trt file exists, it opens the .trt file, imports TensorRT's runtime functionality, and uses deserialize_cuda_engine to load the CUDA engine from the serialized file.\n",
    "\n",
    "In the get_engine() function, a custom build_engine() function is defined. As shown in the code segment, if the .trt file is not found in the directory, this function converts the .onnx file in the directory to the .trt format. Below is a brief explanation of the transformation function:\n",
    "\n",
    "The with statement references several TensorRT functions, builder, network, config, parser, and runtime, in preparation for the format conversion.<br><br>\n",
    ".config.max_workspace_size = 1 << 28 defines the network memory workspace size.<br><br>\n",
    ".builder.max_batch_size = 1 sets the batch size for processing to 1.<br> During training, for efficiency, max_batch_size might be 64 or larger, but for inference, it processes only one image frame at a time.\n",
    "Next, it checks if the .onnx file exists, reads the file, and verifies its correctness.<br><br>\n",
    ".network.get_input(0).shape = [1, 3, 320, 320] specifies the network input format as [batch size=1, color=3 (RGB), image width=320, image height=320].<br><br>\n",
    ".plan = builder.build_serialized_network(network, config) is the format conversion function, which uses TensorRT's API to define and configure the network model. It applies a series of optimization strategies, such as layer fusion and memory management optimization, to enhance the network's execution speed and reduce latency, and saves the optimized network structure and related information into serialized (usually binary) memory.<br><br>\n",
    ".engine = runtime.deserialize_cuda_engine(plan) function restores (or deserializes) the CUDA engine from the serialized memory. The CUDA engine is the neural network used in TensorRT for executing deep learning inference, and it can be directly used for subsequent inference operations.<br><br>\n",
    "Finally, the transformed network data is saved as a .trt file for future use, allowing it to be directly read in later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.context = self.engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd438e",
   "metadata": {},
   "source": [
    "To create a new execution context for the inference engine, configure the input and output bindings for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.inputs, self.outputs, self.bindings, self.stream = common.allocate_buffers(self.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3086e",
   "metadata": {},
   "source": [
    "\n",
    "Allocate memory for the neural network inputs and outputs.<br>\n",
    " The structure and function of each memory allocation will be introduced in the inference section.<br>\n",
    "  Here, the memory allocation function common.allocate_buffers() is defined in common.py and imported at the beginning of the model.py file with the statement:<br>\n",
    "import common<br>\n",
    "With this, the import of the neural network, which is the core content of VAIC's AI, is now complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccaef64",
   "metadata": {},
   "source": [
    "###   2. Configuration and Connection of the Depth Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb78ef",
   "metadata": {},
   "source": [
    "Returning to the overunder.py file, within the__init__(self) function of the class MainApp, to emphasize the core role of the neural network, the establishment of the YOLOv3 network has already been explained. <br>\n",
    "Now, let's introduce the configuration and connection of the external device, the Camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.camera = Camera()\n",
    "self.camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d084d",
   "metadata": {},
   "source": [
    "Execute the initialization function of the Class Camera().\n",
    "\n",
    "Note that in the following code segment, rs indicates the use of the pyrealsense2 library, which is imported at the beginning of overunder.py with the statement:\n",
    "\n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        self.pipeline = rs.pipeline()  # Initialize RealSense pipeline\n",
    "        self.config = rs.config()\n",
    "        # Enable depth stream at 640x480 in z16 encoding at 30fps\n",
    "        self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        # Enable color stream at 640x480 in bgr8 encoding at 30fps\n",
    "        self.config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d99ad",
   "metadata": {},
   "source": [
    "As indicated in the comments, create instances of rs.pipeline() and rs.config().\n",
    "\n",
    "Configure the depth data stream with width=640 pixels, height=480 pixels, data format=z16, and frame rate=30 frames per second.\n",
    "\n",
    "Configure the image data stream with width=640 pixels, height=480 pixels, data format=bgr8, and frame rate=30 frames per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.camera.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca98c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def start(self):\n",
    "       self.profile = self.pipeline.start(self.config)   # Start the pipeline\n",
    "        # Obtain depth sensor and calculate depth scale\n",
    "        depth_sensor = self.profile.get_device().first_depth_sensor()\n",
    "        self.depth_scale = depth_sensor.get_depth_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd384bd",
   "metadata": {},
   "source": [
    "\n",
    ".self.profile = self.pipeline.start(self.config)\n",
    "According to the previously defined depth and image configuration config, set the camera hardware parameters and open the video stream according to the configured parameters.\n",
    "\n",
    "\n",
    ".depth_sensor = self.profile.get_device().first_depth_sensor()\n",
    "Get a depth sensor object from the currently configured RealSense device.\n",
    "\n",
    "\n",
    ".self.depth_scale = depth_sensor.get_depth_scale()\n",
    "Obtain the depth scale of the depth sensor. This scale is a floating-point number representing the conversion from pixel values in the depth image to actual physical distances.\n",
    "\n",
    "self.depth_scale is the parameter used to create self.processing = Processing(self.camera.depth_scale).\n",
    "\n",
    "With this, the setup and startup of the Camera are complete. Through the pipeline, data is transmitted to the Jetson Nano with the depth data stream configured to width=640 pixels, height=480 pixels, data format=z16, and frame rate=30 frames per second, and the image data stream configured to width=640 pixels, height=480 pixels, data format=bgr8, and frame rate=30 frames per second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3225304",
   "metadata": {},
   "source": [
    "###   3. &nbsp; &nbsp;Object Recognition Process of the VAIC Neural Network\n",
    "To introduce the object recognition inference process of the YOLOv3 neural network, we need to return to the main program, which is the entry point for the entire AI control program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = MainApp()  # Create the main application\n",
    "    app.run()  # Run the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277a483",
   "metadata": {},
   "source": [
    "\n",
    "In the initialization of MainApp(), we temporarily ignore:\n",
    "\n",
    ".self.v5 = V5SerialComms()<br>\n",
    ".self.v5Map = MapPosition()<br>\n",
    ".self.v5Pos = V5GPS()<br>\n",
    ".self.v5Web = V5WebData(self.v5Map, self.v5Pos)<br>\n",
    ".self.stats = Statistics(0, 0, 0, 640, 480, 0, False)<br>\n",
    ".self.rendering = Rendering(self.v5Web)<br>\n",
    "app.run() is a loop function. In app.run(), we temporarily ignore:<br>\n",
    ".self.v5.start()<br>\n",
    ".self.v5Pos.start()<br>\n",
    ".self.v5Web.start()<br>\n",
    "This does not mean that these functions are not important.<br>\n",
    " If any of these functions do not execute properly, the system will not work. However, these functions are conventional and auxiliary, and they are not directly related to AI object recognition technology. They will all be explained in detail later.<br>\n",
    "\n",
    "Enter the loop of app.run()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ebdd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddadc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()  # start time of the loop\n",
    "        frames = self.camera.get_frames()\n",
    "        depth_image, color_image, depth_map = self.processing.process_frames(frames)\n",
    "        invoke_time = time.time()\n",
    "        output, detections = self.processing.detect_objects(color_image)\n",
    "        invoke_time = time.time() - invoke_time\n",
    "        aiRecord = self.processing.compute_detections(self, detections, depth_image)\n",
    "        self.set_v5(aiRecord)\n",
    "        self.rendering.set_images(output, depth_map)\n",
    "        self.rendering.set_detection_data(aiRecord)\n",
    "        self.rendering.set_stats(self.stats, self.v5Pos, start_time, invoke_time, run_time)\n",
    "        # self.rendering.display_output(output)\n",
    "finally:\n",
    "    self.camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe455ca6",
   "metadata": {},
   "source": [
    "\n",
    ".start_time = time.time()\n",
    "Saves the start time of each loop.\n",
    "\n",
    ".frames = self.camera.get_frames()\n",
    "Extracts frame images from the pipeline:\n",
    "\n",
    "\n",
    "def get_frames(self):\n",
    "    return self.pipeline.wait_for_frames()\n",
    "wait_for_frames() means waiting for the image frames to refresh and extracting image and distance data between the intervals of two frames.\n",
    "\n",
    "\n",
    ".depth_image, color_image, depth_map = self.processing.process_frames(frames)\n",
    "The function is as follows:\n",
    "\n",
    "Frame alignment involves aligning the depth matrix from infrared ranging with the positions of the transformed color camera image, giving each point in the scene image a depth value. The scene image is in color_image, and the depth is saved in depth_image. Using depth_image, normalization is performed, and a color map (COLORMAP_JET) is applied to generate a colored depth map (depth_map).\n",
    "\n",
    "The function's purpose is to extract the depth image and color image from the aligned frames, then normalize and apply a color map to the depth image, and return these three images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_frames(self, frames):\n",
    "        # Align frames and extract color and depth images\n",
    "        # Apply a color map to the depth image\n",
    "        self.align_frames(frames)\n",
    "        depth_image = np.asanyarray(self.depth_frame_aligned.get_data())\n",
    "        color_image = np.asanyarray(self.color_frame_aligned.get_data())\n",
    "        depthImage = cv2.normalize(depth_image, None, alpha=0.01, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        depth_map = cv2.applyColorMap(depthImage, cv2.COLORMAP_JET)\n",
    "\n",
    "        return depth_image, color_image, depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        invoke_time = time.time()\n",
    "        output, detections = self.processing.detect_objects(color_image)\n",
    "        invoke_time = time.time() - invoke_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c34e63",
   "metadata": {},
   "source": [
    "\n",
    "The difference between two invoke_time instances is the time taken for one object detection.\n",
    ".output, detections = self.processing.detect_objects(color_image)<br>\n",
    "This is the detection function where the input is the color image, and the output is the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def detect_objects(self, color_image):\n",
    "        # Perform object detection and return results using the Model class in model.py\n",
    "        output, detections = self.model.inference(color_image)\n",
    "        return output, detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744b2a9",
   "metadata": {},
   "source": [
    "\n",
    "The function output, detections = self.model.inference(color_image) calls the following function.<br> Inference is the process of forward propagation in a trained neural network, which refers to the neural network's working state. It needs a relatively detailed explanation. <br>Here are all the statements of the function, analyzed one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(self, inputImage):\n",
    "    # Perform inference on the given image and return the bounding boxes, scores, and\n",
    "    # classes of detected objects.\n",
    "\n",
    "    # Process the input image\n",
    "    input_image = Model.image_processing(inputImage)\n",
    "\n",
    "    # Define input resolution and create preprocessor\n",
    "    input_resolution_yolov3_HW = (320, 320)\n",
    "    preprocessor = PreprocessYOLO(input_resolution_yolov3_HW)\n",
    "\n",
    "    # Process the image and get original shape\n",
    "    image_raw, image = preprocessor.process(input_image)\n",
    "    shape_orig_WH = image_raw.size\n",
    "\n",
    "    # Define output shapes for post-processing\n",
    "    output_shapes = [(1, 24, 10, 10), (1, 24, 20, 20)]\n",
    "\n",
    "    # Set the input and perform inference\n",
    "    self.inputs[0].host = image\n",
    "    trt_outputs = common.do_inference_v2(self.context, bindings=self.bindings,\n",
    "                inputs=self.inputs,outputs=self.outputs,stream=self.stream)\n",
    "\n",
    "\n",
    "\n",
    "        # Reshape the outputs for post-processing\n",
    "        trt_outputs = [output.reshape(shape) for output, shape in zip(trt_outputs,\n",
    "                                                                      output_shapes)]\n",
    "\n",
    "        #Define arguments for post-processing\n",
    "        postprocessor_args = {\n",
    "            \"yolo_masks\": [(3, 4, 5), (0, 1, 2)],\n",
    "            \"yolo_anchors\": [(10, 14), (23, 27), (37, 58), (81, 82), (135, 169), (344, 319)],\n",
    "            \"obj_threshold\": [0.4, 0.90, 0.90],  # Different thresholds for each class\n",
    "            #label (Green, Red, Blue)\n",
    "            \"nms_threshold\": 0.5,\n",
    "            \"yolo_input_resolution\": input_resolution_yolov3_HW,\n",
    "        }#\n",
    "\n",
    "        # Perform post-processing\n",
    "        postprocessor = PostprocessYOLO(**postprocessor_args)\n",
    "        boxes, classes, scores = postprocessor.process(trt_outputs, (shape_orig_WH))\n",
    "\n",
    "        Detections = []\n",
    "\n",
    "        # Handle case with no detections\n",
    "        if boxes is None or classes is None or scores is None:\n",
    "            print(\"No objects were detected.\")\n",
    "            return input_image, Detections\n",
    "\n",
    "        # Draw bounding boxes and return detected objects\n",
    "        obj_detected_img = Model.draw_bboxes(image_raw, boxes, scores, classes,\n",
    "                                             ALL_CATEGORIES, Detections)\n",
    "        obj_detected_img = obj_detected_img.convert(\"RGB\")\n",
    "        return np.array(obj_detected_img), Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d83ddb",
   "metadata": {},
   "source": [
    "\n",
    "input_image = Model.image_processing(inputImage)\n",
    "\n",
    "Its purpose is to enhance the image by adjusting its hue, saturation, and brightness.\n",
    "\n",
    ".hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)<br>\n",
    "Converts the image to the HSV color space. This uses OpenCV's cvtColor function to convert the input BGR image (the default format for images read by OpenCV) to the HSV color space. The HSV color space consists of three components: Hue, Saturation, and Value, which are more suitable for processing color information.\n",
    "\n",
    ".hsv[..., 0] = hsv[..., 0] + 12<br>\n",
    "Modifies the hue channel of the HSV image by increasing each pixel value by 12. This typically shifts the overall color of the image towards a particular hue.\n",
    "\n",
    ".hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.2, 0, 255)\n",
    "First multiplies the saturation channel values by 1.2 to increase saturation, then uses np.clip to ensure these values remain within the range of 0 to 255. Increasing saturation usually makes the colors in the image more vivid.\n",
    "\n",
    "hsv[:, :, 2] = np.clip(hsv[:, :, 2] * 1.1, 0, 255)\n",
    "Similarly adjusts the brightness channel by multiplying by 1.1. This increases brightness, but again uses np.clip to ensure values stay within the valid range.\n",
    "\n",
    ".return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)<br>\n",
    "Converts the image back to the BGR color space. After the modifications, the hsv variable now contains the modified HSV image. To display it on standard monitors or applications, it needs to be converted back to BGR format. The cvtColor function is used to convert the HSV image back to BGR format, and the processed image is returned.\n",
    "\n",
    ".input_resolution_yolov3_HW = (320, 320)<br>\n",
    ".preprocessor = PreprocessYOLO(input_resolution_yolov3_HW)<br>\n",
    "\n",
    "Creates an instance of `Class PreprocessYOLO` with a resolution of 320x320, according to the standard input image resolution for YOLOv3. For details, refer to `data_processing.py`.\n",
    "\n",
    ".image_raw, image = preprocessor.process(input_image)<br>\n",
    ".shape_orig_WH = image_raw.size<br>\n",
    "The preprocessor.process() calls _load_and_resize(self, input_image) and _shuffle_and_normalize(self, image) functions, transforming the input image to 320x320 PIL format for inference. image_raw is converted to PIL format and retains its original resolution for dynamic display. For details, refer to data_processing.py.\n",
    ".output_shapes = [(1, 24, 10, 10), (1, 24, 20, 20)]<br>\n",
    "Defines the output format of YOLOv3. This includes the shapes of the YOLOv3 model's outputs. Typically, the YOLOv3 model has multiple output layers, each corresponding to different scales of feature maps.\n",
    "\n",
    "Here, there are two output layers with shapes (1, 24, 10, 10) and (1, 24, 20, 20). These shapes usually mean:\n",
    "\n",
    "The first dimension (1) is the batch size, indicating how many images are processed at once (in this case, 1 image).<br>\n",
    "The second dimension (24) is the number of anchors per layer multiplied by the number of predicted object parameters (object center coordinates x, y, predicted bounding box width w, height h, confidence, and class probabilities for tri-colored balls, 3x(5+3)=24).<br> Anchors will be defined and explained later.\n",
    "The remaining two dimensions (10x10 and 20x20) are the sizes of the feature maps, indicating the number of grids at different scales.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea412ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution_yolov3_HW = (320, 320)\n",
    "preprocessor = PreprocessYOLO(input_resolution_yolov3_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set the input and perform inference\n",
    "        self.inputs[0].host = image\n",
    "        trt_outputs = common.do_inference_v2(self.context, bindings=self.bindings,\n",
    "                     inputs=self.inputs,outputs=self.outputs, stream=self.stream)\n",
    "\n",
    "        # Reshape the outputs for post-processing\n",
    "        trt_outputs = [output.reshape(shape) for output, shape in zip(trt_outputs,\n",
    "                                                        output_shapes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41aeb2",
   "metadata": {},
   "source": [
    "\n",
    "Using the YOLOv3 visual neural network to perform object recognition on real-time acquired scene images:\n",
    "\n",
    "\n",
    ".self.inputs[0].host = image\n",
    "The image is delivered to the CPU memory space allocated for inference. In CUDA programming, host refers to the CPU, and driver refers to the GPU.\n",
    "\n",
    "\n",
    ".trt_outputs = common.do_inference_v2(self.context, bindings=self.bindings, inputs=self.inputs, outputs=self.outputs, stream=self.stream)<br>\n",
    "This calls the common.do_inference_v2() function provided by NVIDIA. trt_outputs are the raw outputs obtained from the YOLOv3 model, typically containing multiple lists or tuples with the model's detection results for the input image. <br>\n",
    "self.context is the YOLOv3 network context introduced earlier. bindings are arrays that bind the data between the host memory (CPU) and the device memory (GPU), ensuring that during inference, input data is correctly transferred to the device and output data is correctly returned to the host.<br>\n",
    "\n",
    "Refer to the common.py file for the do_inference_v2() function, where you'll see a series of computational operations involving the CUDA platform to implement TensorRT. Here is a brief explanation of the calculations:\n",
    "\n",
    "\n",
    ".[cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]<br>\n",
    "Copies the detection image from the CPU to the GPU for executing YOLOv3 network TensorRT forward propagation in CUDA platform programming.\n",
    "\n",
    ".context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)<br>\n",
    "Executes the inference computation.<br>\n",
    "\n",
    ".[cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]<br>\n",
    "Copies the recognition results from the GPU's out.device to the CPU's out.host.<br>\n",
    "\n",
    ".stream.synchronize()<br>\n",
    "Synchronizes data, waiting and checking for all operations on the GPU to complete.\n",
    "\n",
    "It's important to note that the functions executed previously all have _async, meaning they are asynchronous functions. Asynchronous programming is a technique that allows a program to continue performing other tasks while waiting for an operation (such as an I/O operation) to complete, rather than blocking and waiting for the operation to finish. This can significantly improve the responsiveness and throughput of a program, especially when handling operations that involve waiting, such as CPU and GPU coordination, network requests, and file I/O.<br>\n",
    "\n",
    "Calling stream.synchronize() waits for all queued operations on the GPU to complete. This is necessary because although the previous memory copies and inference operations are asynchronous, it is important to ensure they have been properly completed before finishing the inference process.<br>\n",
    "\n",
    ".trt_outputs = [output.reshape(shape) for output, shape in zip(trt_outputs, output_shapes)]<br>\n",
    "Reshapes each element in the trt_outputs list (assuming they are NumPy arrays or similar iterable objects) according to the corresponding shape in the output_shapes list. The reshaped data is collected into a new list and assigned to trt_outputs. Note that this effectively modifies the trt_outputs list in place according to the shapes in output_shapes.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281d660",
   "metadata": {},
   "source": [
    "\n",
    "First, we need to explain the post-processing parameters postprocessor_args, as shown below:\n",
    "\n",
    ".yolo_masks: YOLOv3 uses feature maps of different scales to detect objects of different sizes. Each feature map is responsible for detecting objects of a specific size. yolo_masks defines the indices of the anchors for each feature map. In this example, there are two feature maps.<br>\n",
    " The first feature map uses anchor indices (3, 4, 5), and the second feature map uses anchor indices (0, 1, 2).<br>\n",
    "\n",
    ".yolo_anchors: Anchors are a predefined set of width-height ratios used as the initial shapes for predicting object bounding boxes. YOLOv3 uses these anchors to predict the offsets of the actual bounding boxes relative to these initial shapes. Here, six anchors are defined, each being a tuple of width and height.<br>\n",
    "\n",
    "In simple terms: Anchors are rectangles that approximate the shape of the object to be recognized, distinguishing it from other objects with different width-height ratios. For the tri-colored balls, which have a width-height ratio of 1:1, the camera image is 640x480. Since YOLOv3's input image is 320x320, the width is compressed by half, and the height is compressed to 320/480, making the bounding box of the recognized object rectangular.<br>\n",
    "\n",
    ".obj_threshold: [0.4, 0.90, 0.90] - Threshold for object color judgment; green=0.4, red=0.9, blue=0.9.<br>\n",
    "\n",
    ".nms_threshold: Non-Maximum Suppression (NMS) threshold; NMS is a technique to reduce the number of overlapping bounding boxes. When the overlapping area (Intersection Over Union, IOU) of two bounding boxes is greater than the NMS threshold, the bounding box with the higher confidence is retained, and the one with the lower confidence is suppressed (i.e., deleted). This ensures that a single object is not detected multiple times.<br>\n",
    "\n",
    ".yolo_input_resolution: input_resolution_yolov3_HW, 320x320, the input image size for YOLOv3.\n",
    "\n",
    ".postprocessor = PostprocessYOLO(**postprocessor_args)\n",
    "Creates an instance of Class PostprocessYOLO with the above arrays as parameters.\n",
    "\n",
    ".boxes, classes, scores = postprocessor.process(trt_outputs, (shape_orig_WH))<br>\n",
    "Processes the YOLOv3 outputs generated from TensorRT forward propagation. This post-processes these outputs and returns lists of bounding boxes for the detected objects along with their respective classes and confidence scores, placing them in different lists. The function's boxes, classes, and scores are the three variables unpacked from the return values of the postprocessor.process method.<br>\n",
    "\n",
    "That is, the bounding boxes, classes, and confidence scores extracted from the output of the object detection model. shape_orig_WH contains the original input image's width and height (in pixels). This information is crucial when converting the bounding box coordinates in the model's output from normalized coordinates or relative coordinates back to the original image's coordinates.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397036dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor_args = {\n",
    "    \"yolo_masks\": [(3, 4, 5), (0, 1, 2)],\n",
    "    \"yolo_anchors\": [(10, 14), (23, 27), (37, 58), (81, 82), (135, 169), (344, 319)],\n",
    "    \"obj_threshold\": [0.4, 0.90, 0.90],  # Different thresholds for each class label\n",
    "    #(BlueGreen, Red, )\n",
    "    \"nms_threshold\": 0.5,\n",
    "    \"yolo_input_resolution\": input_resolution_yolov3_HW,\n",
    "}\n",
    "postprocessor = PostprocessYOLO(**postprocessor_args)\n",
    "boxes, classes, scores = postprocessor.process(trt_outputs, (shape_orig_WH))\n",
    "\n",
    "Detections = []\n",
    "\n",
    "# Handle case with no detections\n",
    "if boxes is None or classes is None or scores is None:\n",
    "    print(\"No objects were detected.\")\n",
    "    return input_image, Detections\n",
    "\n",
    "# Draw bounding boxes and return detected objects\n",
    "obj_detected_img = Model.draw_bboxes(image_raw, boxes, scores, classes, ALL_CATEGORIES,\n",
    "                                     Detections)\n",
    "obj_detected_img = obj_detected_img.convert(\"RGB\")\n",
    "return np.array(obj_detected_img), Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0181cb",
   "metadata": {},
   "source": [
    "<img src=\"./image/sigmoid-1.png\" width=\"400\" height=\"300\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23750cbc",
   "metadata": {},
   "source": [
    "    图-1 sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e78f89",
   "metadata": {},
   "source": [
    "\n",
    "Refer to the data_processing.py file for the PostprocessYOLO class and its def process(self, outputs, resolution_raw) function. It sequentially calls the _reshape_output, _process_feats, and _process_feats functions, which transform the recognition tensors, including transposition and reshaping. These functions arrange the recognized objects' information according to the bounding boxes, classes, and confidence scores.<br>\n",
    "\n",
    "A special mention is needed for the calculation of the sigmoid activation function. The sigmoid function is a commonly used activation function in machine learning and deep learning.<br>\n",
    "\n",
    "$$\\\\sigmoid(x) = 1/{(1 + e^{-x})}$$ \n",
    "\n",
    "Especially in logistic regression and neural networks, it can map any real number to a range between 0 and 1. Therefore, it is often used as the activation function of the output layer to interpret the output as a probability. As mentioned earlier, each tri-colored ball has three classifications, and after processing by the sigmoid function, it becomes the probability of each classification. The function graph is shown in Figure-1.<br>\n",
    "\n",
    "Finally, the Model's own draw_bboxes function is called to perform the final processing of the recognized objects.<br>\n",
    ".obj_detected_img = Model.draw_bboxes(image_raw, boxes, scores, classes, ALL_CATEGORIES, Detections)<br>\n",
    "The function is defined as follows. It uses Python's PIL (Python Imaging Library) image processing function library, which is imported at the beginning of the model.py file with the statement: <br>\n",
    "from PIL import ImageDraw<br>\n",
    "\n",
    "It extracts the center and diagonal coordinates of the bounding box for each recognized object and organizes the recognition data of each object into the Detections array. The max and min functions control the bounding box to stay within the image boundary when only part of the object is on the image.\n",
    "\n",
    "After completing draw_bboxes, it calls obj_detected_img = obj_detected_img.convert(\"RGB\") to convert the detected image to RGB format for use in subsequent programs. This completes the recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(image_raw, bboxes, confidences, categories, all_categories, Detections,\n",
    "                bbox_color=\"white\"):\n",
    "        # Draw bounding boxes on the original image and return it.\n",
    "\n",
    "        # Create drawing context\n",
    "        draw = ImageDraw.Draw(image_raw)\n",
    "\n",
    "        # Draw each bounding box\n",
    "        for box, score, category in zip(bboxes, confidences, categories):\n",
    "            x_coord, y_coord, width, height = box\n",
    "            left = max(0, np.floor(x_coord + 0.5).astype(int))\n",
    "            top = max(0, np.floor(y_coord + 0.5).astype(int))\n",
    "            right = min(image_raw.width, np.floor(x_coord + width + 0.5).astype(int))\n",
    "            bottom = min(image_raw.height, np.floor(y_coord + height + 0.5).astype(int))\n",
    "\n",
    "            # Draw the rectangle and text\n",
    "            # draw.rectangle(((left, top), (right, bottom)), outline=bbox_color)\n",
    "            # draw.text((left, top - 12), \"{0} {1:.2f}\".format(all_categories[category], score),\n",
    "            # fill=bbox_color)\n",
    "\n",
    "            # Create and store the raw detection object\n",
    "            raw_detection = rawDetection(int(left), int(top), [x_coord, y_coord], int(width),\n",
    "                                         int(height), score,category)\n",
    "\n",
    "            Detections.append(raw_detection)\n",
    "\n",
    "        return image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c6173",
   "metadata": {},
   "source": [
    "\n",
    "This completes the entire execution process of output, detections = self.processing.detect_objects(color_image), which is the main content of the AI technology introduced in VAIC_23_24.\n",
    "\n",
    "It should be noted that each recognized object is organized into the Detections array in the following data format: <br>\n",
    "left position of the bounding box, top position of the bounding box, [center x coordinate, center y coordinate], bounding box width, bounding box height, confidence score, Recognition class.<br>\n",
    "\n",
    "The next section will introduce how to use the AI-recognized object data combined with the real-time field positioning provided by VEX GPS from the Vex Brain, convert it to world coordinate system coordinates, and feedback to the Brain to control the Vex 5 robot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4303d",
   "metadata": {},
   "source": [
    "###   4.&nbsp; &nbsp;Transmission of Object Recognition Data<br>\n",
    "\n",
    "Returning to the main program in overrunder.py, we first provide additional explanations for the other functions that were previously set aside in the initialization of class MainApp:\n",
    "\n",
    ".self.v5 = V5SerialComms()<br>\n",
    "Establishes the USB channel for transmitting recognized object data from Jetson to Brain and for receiving VexGPS field positioning data from Brain to Jetson.<br>\n",
    ".self.v5Map = MapPosition()<br>\n",
    "Provides the Tait-Bryan transformation matrix for converting recognition data from camera coordinates to physical field coordinates, as well as the calculation parameters and methods for the camera installation position.<br>\n",
    ".self.v5Pos = V5GPS()<br>\n",
    "Creates a V5GPS instance on the Jetson side and creates a thread for receiving GPS data sent from Brain, which is used to run GPS data reading and processing, including establishing connections, data reading, coordinate conversion, and status processing. It also sets local variables for GPS offsets.<br>\n",
    ".self.v5Web = V5WebData(self.v5Map, self.v5Pos)<br>\n",
    "Establishes a web server to transmit updated data in real-time for remote terminals to display the coordinates and categories of recognized objects on the field and the actual game scene in real-time.<br>\n",
    ".self.stats = Statistics(0, 0, 0, 640, 480, 0, False)<br>\n",
    "Initializes statistical attributes such as frame rate (FPS), CPU temperature, video dimensions, runtime, and GPS connection status.<br>\n",
    "self.rendering = Rendering(self.v5Web)<br>\n",
    "Through a web link, sends real-time images of the field and object recognition data from the Jetson side to be synchronously displayed on a remote terminal. As shown in Figure 2.<br>\n",
    "\n",
    "In the V5Comm.py file, the related data structures and methods for the above functionalities are defined for reference when implementing these functions. When explaining the following functions, further details will be provided as necessary.<br>\n",
    "\n",
    "It is necessary to briefly explain the mechanism for receiving GPS data sent from Brain to Jetson and transmitting the aiRecord position data from Jetson to Brain. Both processes use threads.<br>\n",
    "A thread is a real-time running background program that, once started, runs automatically.<br>\n",
    "\n",
    "Apart from sharing the power supply and being mounted on the same robot cart, the transmission and reception of GPS data, and the transmission and reception of AI-recognized object position data, are the only links between the Brain and Jetson systems. <br>\n",
    "The sender of GPS data is Brain, and the receiver is Jetson, while the sender of aiRecord recognized object position data is Jetson, and the receiver is Brain. <br>\n",
    "In the V5Comm.py and V5Position.py files, <br>\n",
    ".self.__thread = threading.Thread(target=self.__run, args=()) <br>\n",
    "is called to create their respective threads that call the thread target function __run(), each with different functionalities.\n",
    "\n",
    "Ignoring the explanations of other auxiliary functions within the threads, the __run() function of V5GPS contains data = self.__ser.read_until(b'\\xCC\\x33'), which indicates receiving data until the termination symbol b'\\xCC\\x33' appears.<br>\n",
    "\n",
    "To briefly explain the data transmission process in the __run() function of V5SerialComms, the program segment for data transmission is as follows:\n",
    "\n",
    "First, the thread reads whether there is a data transmission request from the Brain side.<br>\n",
    "\n",
    ".data = self.__ser.readline().decode(\"utf-8\").rstrip()<br>\n",
    "Reads the command sent from the Brain side from the port. When the command \"AA55CC3301\" appears, it indicates that the Brain side requests data transmission.\n",
    "Ignoring the requests for thread time slices, unlocking, and other routine operations,\n",
    ".self.__ser.write(data)<br>\n",
    "Indicates sending all the data to Brain.\n",
    "The technical details of when and how the Brain side sends the data request command will be explained during the discussion of the Brain-side C++ program.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "    while self.__started:  # Continue reading while thread is started\n",
    "        # Read data from the serial port\n",
    "        data = self.__ser.readline().decode(\"utf-8\").rstrip()\n",
    "        if(data == \"AA55CC3301\"):\n",
    "            self.__detectionLock.acquire()\n",
    "            myPacket = V5SerialPacket(self.__MAP_PACKET_TYPE, self.__detections)\n",
    "            self.__detectionLock.release()\n",
    "            data = myPacket.to_Serial()\n",
    "            self.__ser.write(data)  # Write serialized data to the serial port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183be53",
   "metadata": {},
   "source": [
    "\n",
    "The sending of GPS data is completed by the C++ program running on the Brain, while the thread on the Jetson side receives it. Similarly, ignoring the explanations of other auxiliary functions within the thread, the receiving program segment is as follows:<br>\n",
    ".data = self.__ser.read_until(b'\\xCC\\x33')<br>\n",
    "The thread program reads the port until the start symbol '\\xCC\\x33' appears.<br>\n",
    ".x, y, z, az, el, rot = struct.unpack(b'hhhhhh', data[2:14])<br>\n",
    "Unpacks the position and angle data from the received data packet. <br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4651ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = self.__ser.read_until(b'\\xCC\\x33')\n",
    "    if(len(data) == 16):\n",
    "        self.__frameCount = self.__frameCount + 1\n",
    "        status = data[1]\n",
    "        x, y, z, az, el, rot = struct.unpack('<hhhhhh', data[2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979e3ec",
   "metadata": {},
   "source": [
    "\n",
    "Now, discussing the subsequent processing of the recognized object data, the main loop program app.run() executes the following after completing object recognition with processing.detect_objects(color_image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiRecord = self.processing.compute_detections(self, detections, depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017561c7",
   "metadata": {},
   "source": [
    "\n",
    "As noted in the comments of the def compute_detections(self, v5, detections, depth_image) function in overunder.py, an AIRecord is created, and the detection results are computed using depth and image data.\n",
    "\n",
    "The aiRecord is a data structure that contains control information sent from Jetson to Brain. aiRecord.detections includes the ClassID (category ID), Probability, and depth information for each detected target within the camera's field of view. Additionally, it contains the camera image detection and the target's location information on the field. The program list is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a45d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detections(self, v5, detections, depth_image):\n",
    "        # Create AIRecord and compute detections with depth and image data.\n",
    "        # Each AIRecord contains the ClassID, Probablity, and depth information for each detection\n",
    "        # In addition to the detection's camera image and map position information.\n",
    "        aiRecord = V5Comm.AIRecord(v5.get_v5Pos(), [])\n",
    "        for detection in detections:\n",
    "            depth = self.get_depth(detection, depth_image)\n",
    "            imageDet = V5Comm.ImageDetection(\n",
    "                int(detection.x),\n",
    "                int(detection.y),\n",
    "                int(detection.Width),\n",
    "                int(detection.Height),\n",
    "            )\n",
    "            mapPos = v5.v5Map.computeMapLocation(detection, depth, aiRecord.position)\n",
    "            mapDet = V5Comm.MapDetection(mapPos[0], mapPos[1], mapPos[2])\n",
    "            detect = V5Comm.Detection(\n",
    "                int(detection.ClassID),\n",
    "                float(detection.Prob),\n",
    "                float(depth),\n",
    "                imageDet,\n",
    "                mapDet,\n",
    "            )\n",
    "            aiRecord.detections.append(detect)\n",
    "        return aiRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4111b8",
   "metadata": {},
   "source": [
    "\n",
    "The aiRecord is created with the following line in overunder.py:<br>\n",
    "aiRecord = V5Comm.AIRecord(v5.get_v5Pos(), [])<br>\n",
    "Refer to the definition and initialization of AIRecord in V5Comm.py.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964357f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, position: Position, detections: \"list[Detection]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f0572",
   "metadata": {},
   "source": [
    "\n",
    "The first parameter is the Position obtained by v5.get_v5Pos(), and the second parameter is a list[Detection]. When creating the aiRecord instance, two variables are introduced: the position data from VexGPS and the object recognition data from YOLOv3.\n",
    "\n",
    "As explained in the previous section, the detections array contains multiple objects (tri-colored balls) recognized within the camera's field of view. The following loop calculates the distance for each recognized object.\n",
    "\n",
    "The data obtained from VEXGPS includes:\n",
    "\n",
    "1.frameCount: Data frame count.<br>\n",
    "2.status: Communication status; a normal status indicates reliable data.<br>\n",
    "3.x: X-coordinate of the robot's rotation center on the field.<br>\n",
    "4.y: Y-coordinate of the robot's rotation center on the field.<br>\n",
    "5.z: Z-coordinate of the robot's rotation center on the field.<br>\n",
    "6.azimuth: Azimuth angle, which is the angle measured clockwise along the horizontal plane from a reference direction (usually north or south) to a point. In navigation, the azimuth is used to determine the direction of a point relative to the observer.<br>\n",
    "7.elevation: Elevation angle, which is the angle from the horizon up to the target point. Combined with the azimuth, it can precisely locate celestial bodies or other objects. In aviation and satellite communication, the elevation angle is used to describe the inclination of an aircraft or satellite relative to the horizon.<br>\n",
    "8.rotation: Rotation angle, which is the rotation about the observation axis, indicating the rotation of the axis pointing from the ground plane to the zenith around itself. In photogrammetry, machine vision, and robotics, it is used to correct and adjust the orientation of images and sensors.<br>\n",
    "The use of these data will be detailed during the coordinate transformation calculations.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb07eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.frameCount = frameCount\n",
    "self.status = status\n",
    "self.x = x\n",
    "self.y = y\n",
    "self.z = z\n",
    "self.azimuth = azimuth\n",
    "self.elevation = elevation\n",
    "self.rotation = rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980373aa",
   "metadata": {},
   "source": [
    "\n",
    "In the frame alignment function, the depth (distance from the camera's imaging screen) for each point in the color scene image is already stored in depth_image.\n",
    "\n",
    "The get_depth(detection, depth_image) function extracts the depth data from the depth_image for the center 10% area of the detection's bounding box and calculates the arithmetic mean to use as the distance data for that object (refer to the def get_depth(self, detection, depth_img) function in overunder.py).\n",
    "\n",
    "imageDet represents the image area of the detected object, with the center coordinates (x, y) and the bounding box (Width, Height), which is part of the data transmitted to Brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapPos = v5.v5Map.computeMapLocation(detection, depth, aiRecord.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27aa9cb",
   "metadata": {},
   "source": [
    "\n",
    "Calculate the data of each detected object's world coordinates on the field, which will be transmitted to Brain. Please refer to the def computeMapLocation(self, detection, depth, position) function in V5MapPosition.py.\n",
    "\n",
    "By multiplying the relative position of the detected object on the screen with the transformation matrix of the Brian GPS field position, the result of the matrix multiplication gives the physical position of the detected object in the 3D space of the field relative to the robot.\n",
    "\n",
    "Here is a brief explanation of this function:\n",
    "\n",
    "The computeMapLocation() function starts with setting some constants. The length dimensions and position data for the camera and GPS on the Vex robot are read from camera_offsets.json and gps_offsets.json. Please refer to the__init__(self, v5Map, v5Pos, port=None) function definition in class V5WebData in V5Web.py and the main program self.v5Web = V5WebData(self.v5Map, self.v5Pos). <br>\n",
    "These data are determined by the installation positions of the camera and GPS devices on the robot cart and are pre-measured by the designer and recorded in the above files.<br>\n",
    "\n",
    "Next, a rotation transformation matrix is created. The rotation calculation method composed of the azimuth (Azimuth), elevation (Elevation), and twist (Twist) angles is called the Tait-Bryan rotation composite matrix.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345bc93",
   "metadata": {},
   "source": [
    "\n",
    "The Tait-Bryan rotation composite matrix (also known as the Euler rotation matrix) has applications in multiple fields, especially when describing the rotation of objects or coordinate systems in three-dimensional space. Here are some specific application scenarios:\n",
    "\n",
    "Aerospace and Aeronautical Engineering: In aerospace and aeronautical fields, the attitude of an aircraft is typically described using Tait-Bryan angles (yaw, pitch, and roll). These three angles correspond to the rotation of the aircraft around its vertical, lateral, and longitudinal axes, respectively. Using the Tait-Bryan rotation composite matrix, one can conveniently calculate the direction of the aircraft in different attitudes and perform attitude control and navigation.\n",
    "\n",
    "Robotics: In robotics, the end-effector (such as an arm or gripper) needs to be precisely positioned and rotated in three-dimensional space. The Tait-Bryan rotation composite matrix can calculate the attitude of the end-effector relative to the robot base, enabling precise pose control.\n",
    "\n",
    "Computer Graphics: In computer graphics, the rotation and transformation of objects in a three-dimensional scene is a critical topic. The Tait-Bryan rotation composite matrix can be used to calculate the rotation matrix of objects in three-dimensional space, thereby achieving object rotation and transformation. This has widespread applications in 3D games, virtual reality, and animation.\n",
    "\n",
    "Physical Simulation: In physical simulations, such as molecular dynamics simulations and rigid body dynamics simulations, it is necessary to simulate the movement and rotation of objects in three-dimensional space. The Tait-Bryan rotation composite matrix can calculate the attitude of objects at different times, thereby simulating the movement and rotation process of objects.\n",
    "\n",
    "In summary, the Tait-Bryan rotation composite matrix has extensive applications in describing the rotation of objects or coordinate systems in three-dimensional space. It provides a convenient and efficient way to calculate rotation matrices, enabling precise positioning and rotation control of objects in three-dimensional space.\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "R_{az,el,tw}= <br><br><br>\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\cos(az)\\cos(tw) + \\sin(az)\\sin(el)\\sin(tw) & \\sin(az)\\cos(el) & \\cos(az)\\sin(tw) - \\sin(az)\\sin(el)\\cos(tw) \\\\\n",
    "-\\sin(az)\\cos(tw) + \\cos(az)\\sin(el)\\sin(tw) &\n",
    "\\cos(az)\\cos(el) &\n",
    " -\\sin(az)\\sin(tw) - \\cos(az)\\sin(el)\\cos(tw) \\\\\n",
    "-\\cos(el)\\sin(tw) &\n",
    "\\sin(el) &\n",
    "\\cos(el)\\cos(tw)\n",
    "\\end{bmatrix} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2d819",
   "metadata": {},
   "source": [
    "Calculating the position vector of the target object in the camera coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros(shape=(3, 1))\n",
    "vector[0] = depth * (detection.Center[0] - MAXSCREENX) / REALDIST\n",
    "vector[1] = depth\n",
    "vector[2] = depth * (MAXSCREENY - detection.Center[1]) / REALDIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cecb8",
   "metadata": {},
   "source": [
    "The product of the distance vector and the transformation matrix transforms the vector into world space. By multiplying the relative position of the object on the screen with the robot's perspective information, the result of the matrix multiplication gives the physical position of the recognized object in the 3D space relative to the robot.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapLocation = np.matmul(rot, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c67b20",
   "metadata": {},
   "source": [
    "Overlay the current robot position coordinates on the field and convert them to the world coordinate system. The position coordinates are relative to the camera's center point.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapLocation[0] += position.x\n",
    "mapLocation[1] += position.y\n",
    "mapLocation[2] += position.z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1db7b5",
   "metadata": {},
   "source": [
    "Transform the position coordinates to the robot's pivot center using the camera installation position's x, y, z distances to the robot's pivot center ([CAMERAOFFSETX], [CAMERAOFFSETY], [CAMERAOFFSETZ])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a673afd",
   "metadata": {},
   "outputs": [],
   "source": [
    " cameraOffset = np.array([[CAMERAOFFSETX], [CAMERAOFFSETY], [CAMERAOFFSETZ]])\n",
    "rotatedCameraOffset = np.matmul(rot, cameraOffset)\n",
    "\n",
    "        # Add the adjusted camera offset\n",
    "mapLocation[0] += rotatedCameraOffset[0]\n",
    "mapLocation[1] += rotatedCameraOffset[1]\n",
    "mapLocation[2] -= rotatedCameraOffset[2]  # Subtract Z offse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde1b98",
   "metadata": {},
   "source": [
    "After the calculation, return the position of the robot car's pivot center in the field coordinate system. Finally, return mapPos=(mapLocation[0], mapLocation[1], mapLocation[2]).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35866a88",
   "metadata": {},
   "outputs": [],
   "source": [
    " return mapLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e1e6e",
   "metadata": {},
   "source": [
    "Return to the main program's compute_detections loop:<br>\n",
    "Combine the classification, confidence, distance depth, recognized object's image center coordinates, and bounding box width and height (camera field coordinates), and the recognized object's world (competition field) coordinates into a single detect.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c443a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDet = V5Comm.MapDetection(mapPos[0], mapPos[1], mapPos[2])\n",
    "detect = V5Comm.Detection(\n",
    "    int(detection.ClassID),\n",
    "    float(detection.Prob),\n",
    "    float(depth),\n",
    "    imageDet,\n",
    "    mapDet,\n",
    "                           )\n",
    "    aiRecord.detections.append(detect)\n",
    "    return aiRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b0592",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Back to the main loop of the compute_detections function:\n",
    ".mapDet = V5Comm.MapDetection(mapPos[0], mapPos[1], mapPos[2])  # Array mapDet contains the object's location coordinates (X, Y, Z) on the field.\n",
    "\n",
    ".detect = V5Comm.Detection(<br>\n",
    "&nbsp; &nbsp;int(detection.ClassID),<br>\n",
    "&nbsp; &nbsp;float(detection.Prob),<br>\n",
    "&nbsp; &nbsp;float(depth),<br>\n",
    "&nbsp; &nbsp;imageDet,<br>\n",
    "&nbsp; &nbsp;mapDet,<br>\n",
    "           )<br>\n",
    "Combine the classification, confidence, distance depth, recognized object's image center coordinates and bounding box width and height (camera field coordinates), and the recognized object's world (competition field) coordinates into a single detect.\n",
    "\n",
    ".aiRecord.detections.append(detect)<br>\n",
    "Append the detect to aiRecord.detections.<br>\n",
    "After the loop completes, the classification recognition and location coordinates of the objects recognized in the current camera image field are stored in aiRecord.detections, ready to be sent to Brian via USB connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e98f7",
   "metadata": {},
   "source": [
    "\n",
    "In the main program's app.run(), the method self.set_v5(aiRecord) is responsible for sending the data. Since this involves the reception program on Brian's end written in C++, some preliminary explanation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def set_v5(self, aiRecord):\n",
    "        # Set detection data to the Brain if it is connected but does not set any data if None\n",
    "        if self.v5 is not None:\n",
    "            self.v5.setDetectionData(aiRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e313d24",
   "metadata": {},
   "source": [
    "\n",
    "The set_v5(self, aiRecord) method calls the setDetectionData function in V5Comm.py. As previously mentioned, a thread for sending data to Brian is established through an instance of V5SerialComms.\n",
    "\n",
    "The method self.v5.setDetectionData(self, data: AIRecord) is a function in V5SerialComms that can directly operate on its own thread, with the parameter data being AIRecord.\n",
    "\n",
    "The thread calls the acquire method to attempt to obtain a lock. If the lock is not currently held by another thread (i.e., the lock is free), the thread acquires the lock and safely sends the data.\n",
    "\n",
    "If the lock is already held by another thread, the acquire method will keep the calling thread in a loop until the lock is released, and then it sends the data. Once the data transmission is complete, the lock is immediately released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def setDetectionData(self, data: AIRecord):\n",
    "        # Aquire lock and set detection data\n",
    "        self.__detectionLock.acquire()\n",
    "        self.__detections = data\n",
    "        self.__detectionLock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da698e",
   "metadata": {},
   "source": [
    "\n",
    "###    5.&nbsp; &nbsp;Jetson Remote Display and V5Web.py Program\n",
    "To allow participants to dynamically monitor the competition field during the match, VAIC_23_24 has set up a real-time remote display module. The overrunder.py class Rendering continuously provides real-time data for remote display in the main program competition loop, with network transmission functionality defined in the V5Web.py file. VAIC_23_24 uses the WebSocket method for remote communication (refer to the import statement from websocket_server import WebsocketServer at the beginning of the file).\n",
    "\n",
    "WebSocket communication is a standard protocol for computers and the internet, and the program is designed according to the WebSocket standard.<br>\n",
    " Here is a brief introduction to the basic methods for implementing network transmission:\n",
    "\n",
    ".self.__server.run_forever(True) starts the WebSocket server's event loop, allowing it to begin listening for connections and processing incoming messages.<br>\n",
    "In the WebSocket operating mechanism, a common pattern is for the server to set up callback functions to handle messages received from clients. These callback functions process the logic based on the commands or messages sent by the client and may send the requested data back to the client as a response.<br>\n",
    "__new_client: Called when a new client connects.<br>\n",
    "__client_left: Called when a client disconnects.<br>\n",
    "__message_received: Called when the server receives a message from a client.<br>\n",
    "During the initialization of the class MainApp in the main program, an instance of V5WebData is created with the parameters v5Map and v5Pos to set the CameraOffset for v5Map and the GPSOffset for v5Pos.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197adfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.v5Web = V5WebData(self.v5Map, self.v5Pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ad038",
   "metadata": {},
   "source": [
    "\n",
    "Then, create a WebSocketServer server and define three callback functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197e62b",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "        self.__server = WebsocketServer(host = '0.0.0.0', port = self.__serverPort, loglevel = logging.INFO)\n",
    "        self.__server.set_fn_new_client(self.__new_client)\n",
    "        self.__server.set_fn_client_left(self.__client_left)\n",
    "        self.__server.set_fn_message_received(self.__message_received)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e98909",
   "metadata": {},
   "source": [
    "\n",
    "self.__server.set_fn_new_client(self.__new_client): Creates a remote display client, sends data requests to the server, receives data from the server, and displays real-time dynamic images of the competition field.<br>\n",
    "self.__server.set_fn_client_left(self.__client_left): Monitors the status of the transmission channel and issues an alert when disconnected.<br>\n",
    "__message_received(self.__message_received): Upon receiving commands from the client, it calls the corresponding functions to send the display content to the remote server according to the command.<br>\n",
    "Below is a snippet of the message_received part of the program for a simple explanation:<br>\n",
    "Commands such as g_pos, g_detect, g_stats, g_depth, g_color, get_camera_offset, get_gps_offset, etc., are data request commands sent by the client to the server (VAIC_23_24 program does not provide any explanation for this). As shown below, the corresponding functions are called to return the respective types of data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc299ce",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def __message_received(self, client, server, message):\n",
    "    .......\n",
    "    cmds = message.split(\",\")\n",
    "    for cmd in cmds:\n",
    "        if(cmd == \"g_pos\"):\n",
    "            outData['Position'] = self.__getPositionElement()\n",
    "        elif(cmd == \"g_detect\"):\n",
    "            outData['Detections'] = self.__getDetectionElement()\n",
    "        elif(cmd == \"g_stats\"):\n",
    "            outData['Stats'] = self.__getStatsElement()\n",
    "        elif(cmd == \"g_depth\"):\n",
    "            outData['Depth'] = self.__getDepthElement()\n",
    "        elif(cmd == \"g_color\"):\n",
    "            outData['Color'] = self.__getColorElement()\n",
    "        elif(cmd == \"get_camera_offset\"):\n",
    "            outData['CameraOffset'] = self.__getCameraOffset().__dict__\n",
    "        elif(cmd == \"get_gps_offset\"):\n",
    "            outData['GpsOffset'] = self.__getGpsOffset().__dict__\n",
    "    outData = self.convert_numpy_to_list(outData)\n",
    "    server.send_message(client, json.dumps(outData))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016f9d7",
   "metadata": {},
   "source": [
    "\n",
    "From the program's operation process, it can be seen that the Jetson end recognizes the coordinate positions and distances of objects on the field through a neural network. The data transmission thread sends this data to Brian in real time. Real-time data needs to be placed in a shared space. To prevent conflicts between simultaneous writing and reading of data, the main program starts by creating interlocked variables with self.v5Web.start():\n",
    "\n",
    "self.__detections: Position and distance data of the recognized objects.<br>\n",
    "self.__colorImage: Color image.<br>\n",
    "self.__depthImage: Depth image.<br>\n",
    "self.__stats: Statistical data.<br>\n",
    "self.__dataLock: Interlock variable.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c630940",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "    self.__detections = AIRecord(Position(0, 0, 0, 0, 0, 0, 0, 0) [])\n",
    "    self.__colorImage = None\n",
    "    self.__depthImage = None\n",
    "    self.__stats = Statistics(0, 0, 0, 0, 0, 0, False)\n",
    "    self.__dataLock = Lock()\n",
    "    self.__server.run_forever(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66d92d",
   "metadata": {},
   "source": [
    "In the main program's run method, these variables are refreshed in each loop iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e0411",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "    self.set_v5(aiRecord)\n",
    "    self.rendering.set_images(output, depth_map)\n",
    "    self.rendering.set_detection_data(aiRecord)\n",
    "    self.rendering.set_stats(self.stats, self.v5Pos, start_time, invoke_time, run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601dd91a",
   "metadata": {},
   "source": [
    "For example, in the run() method, the current color and depth images of the field of view are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c3a62",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "self.rendering.set_images(output, depth_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbd149",
   "metadata": {},
   "source": [
    "\n",
    "Using the methods from the Rendering class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_images(self, output, depth_image):\n",
    "    # Update web data with color and depth images\n",
    "    self.web_data.setColorImage(output)\n",
    "    self.web_data.setDepthImage(depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27582bb9",
   "metadata": {},
   "source": [
    "Calling the functions from V5Web, note that the defined interlock __dataLock is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setColorImage(self, image):\n",
    "    # Updates the color image data\n",
    "    self.__dataLock.acquire()\n",
    "    self.__colorImage = image\n",
    "    self.__dataLock.release()\n",
    "\n",
    "def setDepthImage(self, image):\n",
    "    # Updates the depth image data\n",
    "    self.__dataLock.acquire()\n",
    "    self.__depthImage = image\n",
    "    self.__dataLock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2f474",
   "metadata": {},
   "source": [
    "This is because __message_received() also retrieves data from these spaces when sending image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    elif(cmd == \"g_depth\"):\n",
    "        outData['Depth'] = self.__getDepthElement()\n",
    "    elif(cmd == \"g_color\"):\n",
    "        outData['Color'] = self.__getColorElement()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0dadb",
   "metadata": {},
   "source": [
    "To avoid conflicts between simultaneous data refresh and retrieval, both data storage and retrieval use an interlock mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c137f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getColorElement(self):\n",
    "        # Returns the color image data encoded in base64\n",
    "        outData = {}\n",
    "        imageData = {}\n",
    "\n",
    "        self.__dataLock.acquire()\n",
    "        pixelData = self.__colorImage\n",
    "        self.__dataLock.release()\n",
    "\n",
    "        if(len(pixelData) > 0):\n",
    "            imageData['Valid'] = True\n",
    "            imageData['Width'] = pixelData.shape[1]\n",
    "            imageData['Height'] = pixelData.shape[0]\n",
    "            imageData['Data'] = base64.b64encode(np.array(cv2.imencode(\".jpeg\", pixelData)[1]).tobytes()).decode('utf-8')\n",
    "        else:\n",
    "            imageData['Valid'] = False\n",
    "            imageData['Error'] = \"Image Unavailable\"\n",
    "\n",
    "        outData['Image'] = imageData\n",
    "\n",
    "        return outData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb94b6a",
   "metadata": {},
   "source": [
    "The V5Web.py file also defines the statistical data that needs to be monitored on-site, such as CPU temperature, video resolution, working time, GPS connection status, etc. This is shown in the class Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba574ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, fps: float, invokeTime: float, cpuTemp: float, videoWidth: int,\n",
    "                 videoHeight: int, runTime: int, gpsConnected: bool):\n",
    "    # Initialize statistics attributes such as FPS, CPU temperature, video dimensions, runtime, and GPS connection status\n",
    "    self.fps = fps\n",
    "    self.invokeTime = invokeTime\n",
    "    self.cpuTemp = cpuTemp\n",
    "    self.videoWidth = videoWidth\n",
    "    self.videoHeight = videoHeight\n",
    "    self.runTime = runTime\n",
    "    self.gpsConnected = gpsConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670f906",
   "metadata": {},
   "source": [
    "When the client makes a data request, the server sends this statistical data to the client.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif(cmd == \"g_stats\"):\n",
    "    outData['Stats'] = self.__getStatsElement()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac54b4a",
   "metadata": {},
   "source": [
    "When preparing the data transmission packet, the interlock mechanism is also used:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __getStatsElement(self):\n",
    "        # Returns the current statistics elements\n",
    "        outData = {}\n",
    "\n",
    "        self.__dataLock.acquire()\n",
    "        nowStats = self.__stats\n",
    "        self.__dataLock.release()\n",
    "\n",
    "        outData['FPS'] = nowStats.fps\n",
    "        outData['InferTime'] = nowStats.invokeTime\n",
    "        outData['VideoWidth'] = nowStats.videoWidth\n",
    "        outData['VideoHeight'] = nowStats.videoHeight\n",
    "        outData['RunTime'] = nowStats.runTime\n",
    "        outData['GPSConnected'] = nowStats.gpsConnected\n",
    "        outData['CPUTempurature'] = nowStats.cpuTemp\n",
    "\n",
    "        return outData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4defeb",
   "metadata": {},
   "source": [
    "The VAIC_23_24 software project defines various commands for client data requests but does not provide any technical documentation on the remote client operating mechanism. Participants need to program the client-side application themselves.\n",
    "\n",
    "The remote display screen of the client is shown in Figure-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ae8cd",
   "metadata": {},
   "source": [
    "<img src=\"./image/实况.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114310ba",
   "metadata": {},
   "source": [
    "\n",
    "Figure-2: Real-time field view and object recognition position and distance data displayed on the remote terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c447fa",
   "metadata": {},
   "source": [
    "###    6.&nbsp; &nbsp;The Jetson receives GPS data from the Brian side with the following program:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22b3fd",
   "metadata": {},
   "source": [
    "The class V5GPS in V5Position.py is the functional module for Jetson to receive Vex GPS data sent from Brian.\n",
    "\n",
    "After creating an instance of V5GPS, execute the start(self) function to start the receiving thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def start(self):\n",
    "        # Starts the GPS thread\n",
    "        self.__started = True\n",
    "        self.__thread = threading.Thread(target=self.__run, args=())\n",
    "        self.__thread.daemon = True\n",
    "        self.__thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c96404",
   "metadata": {},
   "source": [
    "\n",
    "The target function def __run(self) of the thread is used for reading and processing GPS data, including establishing a connection, reading data, coordinate transformation, and status processing. It also sets local variables for GPS offsets. Here, we will only explain the data reception part. The GPS data format transmitted from Brian is as follows:<br>\n",
    "\n",
    "frameCount = frameCount<br>\n",
    "status = status<br>\n",
    "x = x<br>\n",
    "y = y<br>\n",
    "z = z<br>\n",
    "azimuth = azimuth<br>\n",
    "elevation = elevation<br>\n",
    "rotation = rotation<br>\n",
    "From the following fragment of the thread's target program, it can be seen that the data length is 16 bytes, and the data header is \"xCC, x33\". In the C++ program on Brian for sending Vex GPS data, which we will introduce below, we will find the same data header and the corresponding data structure mentioned above.\n",
    "\n",
    "After reading the header, the thread increments the received frame count by one, then reads two bytes for the status and the corresponding three two-byte position coordinates and three two-byte angular values.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    while self.__started:\n",
    "        # Read data from the serial port\n",
    "        data = self.__ser.read_until(b'\\xCC\\x33')\n",
    "        if(len(data) == 16):\n",
    "            self.__frameCount = self.__frameCount + 1\n",
    "            status = data[1]\n",
    "            x, y, z, az, el, rot = struct.unpack('<hhhhhh', data[2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f7d17",
   "metadata": {},
   "source": [
    "\n",
    "After dimensional transformation of the coordinates and conversion of the angles to radians, the azimuth angle is used to calculate the new offset. Then, the received data is stored in the shared space self.__position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if(status == 20):\n",
    "        x, y = self.__filter.update(x, y)\n",
    "        self.__positionLock.acquire()\n",
    "        self.__position.x = x\n",
    "        self.__position.y = y\n",
    "        self.__position.z = z\n",
    "        self.__position.azimuth = az\n",
    "        self.__position.elevation = el\n",
    "        self.__position.rotation = rot\n",
    "        self.__position.status = localStatus\n",
    "        self.__position.frameCount = self.__frameCount\n",
    "        self.__positionLock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4d80a",
   "metadata": {},
   "source": [
    "\n",
    "What needs to be mentioned here is that the program performs smoothing on the input x and y coordinates:<br>\n",
    "x, y = self.__filter.update(x, y)<br>\n",
    "V5Position.py imports the definition of class LiveFilter from filter.py at the beginning of the file:<br>\n",
    "from filter import LiveFilter<br>\n",
    "During the initialization of the class MainApp in the main program, an instance self.v5Pos = V5GPS() is created.\n",
    "\n",
    "In the__init__(self, port=None) method of V5GPS, the filter is created with a window size of 10:<br>\n",
    "\n",
    "self.__filter = LiveFilter(10)<br>\n",
    "from collections import deque<br>\n",
    "self.x_buffer = deque(maxlen=window_size)<br>\n",
    "self.y_buffer = deque(maxlen=window_size)<br>\n",
    "In Python, deque is a double-ended queue class from the collections module.<br> \n",
    "It supports fast additions and deletions from both ends. When the number of elements in the deque reaches the window_size, adding a new element will automatically remove the oldest element in the queue.<br>\n",
    "\n",
    "This way, the x_buffer and y_buffer hold the most recent window_size coordinate values. The update(self, x, y) method calculates their average and then sends it to the shared space for the x and y coordinates.<br>\n",
    "\n",
    "In other words, the so-called current plane position coordinates are the average of the last ten GPS measurement coordinates. This smoothing of data effectively avoids GPS measurement errors and eliminates drastic data changes, allowing the robot controlled by this data to move smoothly and fluidly.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class LiveFilter:\n",
    "    def __init__(self, window_size=5, output_file='filtered_data_simple.txt'):\n",
    "        self.window_size = window_size\n",
    "        self.x_buffer = deque(maxlen=window_size)\n",
    "        self.y_buffer = deque(maxlen=window_size)\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def update(self, x, y):\n",
    "        # Update x and y buffers\n",
    "        self.x_buffer.append(x)\n",
    "        self.y_buffer.append(y)\n",
    "\n",
    "        # Compute the average of the current buffer\n",
    "        filtered_x = np.mean(self.x_buffer)\n",
    "        filtered_y = np.mean(self.y_buffer)\n",
    "\n",
    "        # Write the filtered data to an output file\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(f\"{filtered_x}, {filtered_y}\\n\")\n",
    "\n",
    "        return filtered_x, filtered_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90497d9a",
   "metadata": {},
   "source": [
    "self.__position is the shared space where the receiving thread writes data and the main program reads it using get_v5Pos(). To avoid data read/write conflicts caused by simultaneous writing and reading, an interlock mechanism is used when the thread writes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ea802",
   "metadata": {},
   "outputs": [],
   "source": [
    "    self.__positionLock.acquire()\n",
    "    self.__position.x = x\n",
    "    self.__position.y = y\n",
    "    self.__position.z = z\n",
    "    self.__position.azimuth = az\n",
    "    self.__position.elevation = el\n",
    "    self.__position.rotation = rot\n",
    "    self.__position.status = localStatus\n",
    "    self.__position.frameCount = self.__frameCount\n",
    "    self.__positionLock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d25ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def getPosition(self):\n",
    "        # Retrieves the current position\n",
    "        self.__positionLock.acquire()\n",
    "        nowPosition = self.__position\n",
    "        self.__positionLock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992679c",
   "metadata": {},
   "source": [
    "###    7.&nbsp; &nbsp;Detailed Explanation of the Brian-side C++ Control Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccefc09",
   "metadata": {},
   "source": [
    "The Brian-side C++ control program for VAIC_23_24 consists of several key components. This explanation builds upon the earlier brief introduction and refers to the \"Introduction to VAIC Software\".<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287853",
   "metadata": {},
   "source": [
    "#####   7-1.  Comparison of Jetson and Brian-side Communication Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79589445",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To facilitate communication with the Jetson, the Brian-side has added several data structures in ai_jetson.h and a namespace called ai. This namespace defines all the properties and methods needed for data reception, including constants, variables, and functional functions. These will be explained as needed in the following discussion.\n",
    "\n",
    "Before explaining the Brian-side C++ program, let's first look at the data structure used for communication between both sides. Clearly, to ensure smooth communication, the communication data structures on both sides should be as similar as possible.\n",
    "\n",
    "On the Jetson side, the GPS data received from Brian is represented as Position. See V5Comm.py's class Position, which has the following attribute format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.frameCount = frameCount\n",
    "        self.status = status\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.azimuth = azimuth\n",
    "        self.elevation = elevation\n",
    "        self.rotation = rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13fbbf1",
   "metadata": {},
   "source": [
    "The data format for sending GPS data from Brian is defined in the file ai_jetson.h in the V5Example directory as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330c937",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct {\n",
    "\tint32_t\t\tframecnt;\n",
    "\tint32_t\t\tstatus;\n",
    "\tfloat\t    x, y, z;\n",
    "\tfloat\t    az;\n",
    "    float       el;\n",
    "    float       rot;\n",
    "} POS_RECORD;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78ab92",
   "metadata": {},
   "source": [
    "\n",
    "Since the Python language does not require explicit data type specifications for variables, the essence of the data structure is essentially the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292a7f1",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, the recognition data sent from Jetson to Brian is defined on the Python side as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f83357",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = V5Comm.Detection(\n",
    "            int(detection.ClassID),\n",
    "            float(detection.Prob),\n",
    "            float(depth),\n",
    "            imageDet,\n",
    "            mapDet,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91632406",
   "metadata": {},
   "source": [
    "The definition of imageDet is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "            imageDet = V5Comm.ImageDetection(\n",
    "                int(detection.x),\n",
    "                int(detection.y),\n",
    "                int(detection.Width),\n",
    "                int(detection.Height),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df51e5",
   "metadata": {},
   "source": [
    "\n",
    "Additionally,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312feccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDet = V5Comm.MapDetection(mapPos[0], mapPos[1], mapPos[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a93e46",
   "metadata": {},
   "source": [
    "On the Brian side, the structures for receiving data are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071a81f",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct {\n",
    "  int32_t\t\t      classID;\t        // The class ID of the object.\n",
    "  float           probability;      // The probability that this object is in this class (1.0 == 100%)\n",
    "  float           depth;            // The depth of this object in meters from the camera\n",
    "  IMAGE_DETECTION   screenLocation;   // The screen coordinates of this object\n",
    "  MAP_DETECTION     mapLocation;      // The field coordinates of this object\n",
    "} DETECTION_OBJECT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c636b",
   "metadata": {},
   "source": [
    "Image Detection and Map Detection Data Structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca3fd8",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct {\n",
    "\tint32_t\t\tx, y;\n",
    "\tint32_t\t\twidth, height;\n",
    "} IMAGE_DETECTION;\n",
    "typedef struct {\n",
    "  float     x;\n",
    "  float     y;\n",
    "  float     z;\n",
    "} MAP_DETECTION;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1a05e",
   "metadata": {},
   "source": [
    "Similarly, the data format is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6a697",
   "metadata": {},
   "source": [
    "\n",
    "The final format for data exchange between both sides is AI_RECORD.\n",
    "\n",
    "On the Jetson side, when creating an AIRecord instance, it is defined as a combination of class Position and class Detection. list[Detection] indicates that it is a list of Detection objects.\n",
    "\n",
    "In Python, lists can be expanded at any time, so there is no need to limit the dimensions of list[Detection]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a888603",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, position: Position, detections: \"list[Detection]\"):\n",
    "        self.position = position\n",
    "        self.detections = detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552914d1",
   "metadata": {},
   "source": [
    "On the Brian side, AI_RECORD is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1853f2",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct {<br>\n",
    "    int32_t\t\t    detectionCount;\n",
    "    POS_RECORD\t        pos;\n",
    "    DETECTION_OBJECT\tdetections[MAX_DETECTIONS];\n",
    "\n",
    "} AI_RECORD;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5adaa",
   "metadata": {},
   "source": [
    "\n",
    "Here, DETECTION_OBJECT detections[MAX_DETECTIONS] is defined as an array. Since the size of an array cannot be expanded dynamically in C++, it is limited by MAX_DETECTIONS.\n",
    "\n",
    "Comparing the two, the Brian-side structure has an additional detectionCount to count the recognized objects. This is because, in Python, you can directly use len(detections) to get the number of recognized objects, so there is no need to set this count. However, in C++, you need to know the number of recognized objects beforehand to establish the extraction loop.\n",
    "\n",
    "To accommodate this feature of C++, the Jetson side adds this count when packaging the data before sending it.\n",
    "\n",
    "See the V5Comm.py class AIRecord method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccced50",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def to_Serial(self):\n",
    "        # Convert AIRecord properties to serialized binary format\n",
    "        data = struct.pack('<i', len(self.detections))\n",
    "        data += self.position.to_Serial()\n",
    "        for det in self.detections:\n",
    "            data += det.to_Serial()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc0f35",
   "metadata": {},
   "source": [
    "\n",
    ".data = struct.pack('<i', len(self.detections)), packs the number of detections in detections into data as an int.\n",
    "\n",
    ".data += self.position.to_Serial(), packs the position data into data.\n",
    "\n",
    "for det in self.detections:<br>\n",
    "&nbsp; &nbsp;data += det.to_Serial()<br>\n",
    "Iterates through detections and packs all detect objects into data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983559c",
   "metadata": {},
   "source": [
    "#####   7-2.  Brian-side Data Reception Process from Jetson sent<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b18412",
   "metadata": {},
   "source": [
    "\n",
    "The main program executes ai::jetson, jetson_comms; to create an instance of Jetson, and the instance initializes and starts a communication thread called Brian for receiving the target position information from Jetson. <br>\n",
    "The thread function is receive_task and has a high priority.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a8490",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "jetson::jetson() {\n",
    "    state = jetson_state::kStateSyncWait1;\n",
    "    thread t1 = thread( receive_task, static_cast<void *>(this) );\n",
    "    t1.setPriority(thread::threadPriorityHigh);\n",
    "}\n",
    "jetson::~jetson() {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05158aff",
   "metadata": {},
   "source": [
    "\n",
    "When Brian needs new position data, it calls the jetson_comms.request_map() function. Brian sends a request to Jetson for new position data with the request code \"AA55CC3301\". <br>\n",
    "As mentioned earlier, when Jetson receives the request, it can then send the latest position data. The Brian-side thread receive_task remains in a running state, waiting to receive the transmitted data packets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ad9b5",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "int\n",
    "jetson::receive_task( void *arg ) {\n",
    "    if( arg == NULL)\n",
    "      return(0);\n",
    "    jetson *instance = static_cast<jetson *>(arg);\n",
    "    while(1) {\n",
    "      int rxchar = getchar();\n",
    "      if( rxchar >= 0 ) {\n",
    "        instance->total_data_received++;\n",
    "        while( instance->parse( (uint8_t)rxchar ) )\n",
    "          this_thread::yield();\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc1a12",
   "metadata": {},
   "source": [
    "In the loop while( instance->parse( (uint8_t)rxchar ) ), the program reads the incoming information transmitted by Jetson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6f0b8",
   "metadata": {},
   "source": [
    "\n",
    "The parse() function reads the position data sent from Jetson. Referring to jetson::parse(uint8_t data) in ai_jetson.cpp, the function initially keeps reading and checking for the sequence \"0xAA, 0x55, 0xCC, 0x33\", which serves as a header or start of packet identifier.\n",
    "\n",
    "It also performs a CRC32 (Cyclic Redundancy Check 32) algorithm to check the integrity of the data. Once the CRC check is successful, it confirms that the transmitted data is correct and proceeds to the kStateGoodPacket state.\n",
    "\n",
    "Here, we have extracted only the segment of the parse() function within the receive_task thread that receives AI_RECORD data. Apparently:\n",
    "\n",
    ".memcpy(&newMap, &payload.bytes[0], MAP_POS_SIZE); extracts the Position data from the start of the received data packet.<br>\n",
    ".memcpy(&newMap.detections, &payload.bytes[MAP_POS_SIZE], sizeof(DETECTION_OBJECT) * newMap.detectionCount) extracts the individual detection data from the position after the Position data in the received packet.<br>\n",
    ".if(newMap.detectionCount > MAX_DETECTIONS) newMap.detectionCount = MAX_DETECTIONS; If the amount of incoming data exceeds the reserved space for receiving data on the Cpp side, the subsequent data will be discarded.<br>\n",
    ".memcpy(&last_map, &newMap, sizeof(AI_RECORD)); copies the contents of the newMap variable within the thread to the global variable last_map in the main program.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21c5ff",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "     kStateGoodPacketcase jetson_state:::\n",
    "        if( payload_type == MAP_PACKET_TYPE ) {\n",
    "          AI_RECORD newMap;\n",
    "          // Parse the payload packet into a AI_RECORD\n",
    "          memset(&newMap, 0, sizeof(newMap));\n",
    "          memcpy(&newMap, &payload.bytes[0], MAP_POS_SIZE);\n",
    "          if(newMap.detectionCount > MAX_DETECTIONS)\n",
    "            newMap.detectionCount = MAX_DETECTIONS;\n",
    "          memcpy(&newMap.detections, &payload.bytes[MAP_POS_SIZE],\n",
    "                       sizeof(DETECTION_OBJECT) * newMap.detectionCount);\n",
    "          // lock access to last_map and copy data\n",
    "          maplock.lock();\n",
    "          memcpy( &last_map, &newMap, sizeof(AI_RECORD));\n",
    "          maplock.unlock();\n",
    "        }\n",
    "        // timestamp this packet\n",
    "        last_packet_time = timer.system();\n",
    "        packets++;\n",
    "        state = jetson_state::kStateSyncWait1;\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508ee61",
   "metadata": {},
   "source": [
    "\n",
    "The Brian side of the VAIC_23_24 project provides a programming framework for the VEX AI competition, where the specific competition strategy and action planning are determined by the participants through their own programming.\n",
    "\n",
    "Looking at main.cpp, the autonomous program is divided into two parts: the isolated auto_Isolation() and the interactive auto_Interaction().\n",
    "\n",
    ".Competition.autonomous(autonomousMain); initiates the autonomous program. Since firstAutoFlag is set to true, the program will first execute auto_Isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29226b80",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "bool firstAutoFlag = true;\n",
    "void autonomousMain(void) {\n",
    "  if(firstAutoFlag)\n",
    "    auto_Isolation();\n",
    "  else\n",
    "    auto_Interaction();\n",
    "  firstAutoFlag = false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44feb3d9",
   "metadata": {},
   "source": [
    "\n",
    "auto_Isolation(void) demonstrates a simple competition action sequence:\n",
    "\n",
    ".GPS.calibrate(); initiates the calibration of the GPS sensor.<br>\n",
    ".getObject(), which performs a series of function calls getObject()->findTarget()->get_data(), retrieves the position data received by the receive_task thread and copied into last_map.<br>\n",
    "The get_data(AI_RECORD *map) function is as follows:\n",
    "(Note: The actual implementation of get_data(AI_RECORD *map) is not provided in the snippet, but based on the context, it likely extracts data from the AI_RECORD structure pointed to by map.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfb3d8",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "int32_t\n",
    "jetson::get_data( AI_RECORD *map ) {\n",
    "    int32_t length = 0;\n",
    "    if( map != NULL ) {\n",
    "        maplock.lock();\n",
    "        memcpy( map, &last_map, sizeof(AI_RECORD));\n",
    "        length = last_payload_length;\n",
    "        maplock.unlock();\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d327e09",
   "metadata": {},
   "source": [
    "\n",
    "After that, there are simple actions such as opening the arm, picking up the ball, and pushing it into the goal, as indicated by the comments. <br>\n",
    "Once auto_Isolation(void) completes, it proceeds to auto_Interaction(). auto_Interaction() is an empty function intended to be designed and programmed by the competitors.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f982139",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "void auto_Interaction(void) {\n",
    "  // Functions needed: evaluate which ball detected is target, go to target (x,y), intake ball, dump ball,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f68aa",
   "metadata": {},
   "source": [
    "\n",
    "In the main() function of the Brian side, the loop represents the actual competition process, where competitors design their competition strategies and implement them through programming. This simple demonstration provides three essential functions required during the competition:\n",
    "\n",
    "1.Obtain the target's position data from the Jetson side, and based on the competition strategy, select a target, move towards it, and complete the competition actions for the current phase. jetson_comms.get_data( &local_map );<br>\n",
    "2.If necessary, send essential information to other friendly robots. In this case, the chosen information to send is the local robot's position coordinates and heading angle. However, the format can be adjusted to send other information that facilitates implementing the competition strategy. link.set_remote_location( local_map.pos.x, local_map.pos.y, local_map.pos.az, local_map.pos.status );<br>\n",
    "3.Send a request to the Jetson side for the location coordinates of a new target, to continue with the next phase of the competition. jetson_comms.request_map();<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad4c77",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "  while(1) {\n",
    "\n",
    "      jetson_comms.get_data( &local_map );\n",
    "      link.set_remote_location( local_map.pos.x, local_map.pos.y, local_map.pos.az, local_map.pos.status );\n",
    "      jetson_comms.request_map();\n",
    "      this_thread::sleep_for(loop_time);\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b11a9",
   "metadata": {},
   "source": [
    "\n",
    "Here is a brief comparison and explanation of how the request_map() function interacts with the Jetson's running thread in terms of data requirements. The C++ source code for request_map() is as follows:\n",
    "\n",
    "Brian communicates through writing to a file, while the Jetson's communication port is \"/dev/serial1\", and the data request command is \"AA55CC3301\".\n",
    "\n",
    "On the Brian side, to send the request command, it uses fwrite(msg, 1, strlen(msg), fp);."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5ff88",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "void\n",
    "jetson::request_map() {\n",
    "    if( state != jetson_state::kStateSyncWait1 && timer.time() > 250 ) {\n",
    "      state = jetson_state::kStateSyncWait1;\n",
    "    }\n",
    "    if( state == jetson_state::kStateSyncWait1 ) {\n",
    "      FILE *fp = fopen(\"/dev/serial1\", \"w\");\n",
    "      static char msg[] = \"AA55CC3301\\r\\n\";\n",
    "      fwrite( msg, 1, strlen(msg), fp );\n",
    "      fclose(fp);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf2d64",
   "metadata": {},
   "source": [
    "When the Jetson thread receives a data request, it sends the location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while self.__started:  # Continue reading while thread is started\n",
    "  # Read data from the serial port\n",
    "  data = self.__ser.readline().decode(\"utf-8\").rstrip()\n",
    "    if(data == \"AA55CC3301\"):\n",
    "      self.__detectionLock.acquire()\n",
    "      myPacket = V5SerialPacket(self.__MAP_PACKET_TYPE, self.__detections)\n",
    "      self.__detectionLock.release()\n",
    "      data = myPacket.to_Serial()\n",
    "      self.__ser.write(data)  # Write serialized data to the serial port\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e34b9a",
   "metadata": {},
   "source": [
    "###   8.&nbsp; &nbsp;Explanation of the ai_robot_link.cpp Program for the Brian Side of the Vex5 Robot<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e934e09",
   "metadata": {},
   "source": [
    "\n",
    "In the C++ program on the Brian side, VAIC_23_24 introduces a real-time information exchange mechanism called ai_link between two competing robots. The communication between the two robots differs from the previously mentioned Jetson-to-Brian communication as the dual-robot system uses a wireless mode, while the Jetson-to-Brian communication utilizes a wired USB interface.\n",
    "\n",
    "To enable this wireless communication, the robot_link class has been defined in the ai namespace, inheriting from vex::serial_link. The vex::serial_link class encapsulates the wireless communication functionality provided by Vex. More details about this class can be found in the ai_robot_link.h header file.\n",
    "\n",
    "The VAIC_23_24 competition requires participants to enter the auto_Interaction mode, but does not provide any reference examples for dual-robot interaction, allowing competitors to unleash their creativity in designing the competition setup. However, it does provide sufficient basic functional programming examples for programmers to refer to.\n",
    "\n",
    "Here, as a prerequisite for introducing the main operational mechanism of ai_robot_link, a brief explanation of some defined variables and function functionalities is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b14af",
   "metadata": {},
   "source": [
    "#### 1. Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e4b66",
   "metadata": {},
   "source": [
    "The data packet header, as indicated in the comments, contains: synchronization code, packet length, packet type, and cyclic redundancy check (CRC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac5460",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct __attribute__((__packed__)) _packet_header {\n",
    "    uint8_t    sync[2];    // synchronizing bytes, find the start of a valid packet\n",
    "    uint8_t    length;     // length of map record payload, does not include header\n",
    "    uint8_t    type;       // type of packet,\n",
    "    uint16_t   crc;        // crc32 of payload, this may need to be removed to allow more payload\n",
    "} packet_header;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4109d",
   "metadata": {},
   "source": [
    "Synchronization code: A two-byte constant used as an identifier to recognize the starting position of the transmitted information packet.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1a7ac",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    " enum class sync_byte {\n",
    "    kSync1 = 0xA5,\n",
    "    kSync2 = 0x5A\n",
    " };"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e2edd",
   "metadata": {},
   "source": [
    "Payload: The valid data carried in the data packet. In this case, it is simply set to the robot's position (X, Y) and its orientation (angle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffc151",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct __attribute__((__packed__)) _packet_1_payload {\n",
    "    float      loc_x;\n",
    "    float      loc_y;\n",
    "    float      heading;\n",
    "} packet_1_payload;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd25f5f",
   "metadata": {},
   "source": [
    "The structure of the data packet used during communication transmission is packet_1_t = packet_header + packet_1_payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5291c1",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "typedef struct __attribute__((__packed__)) _packet_1_t {\n",
    "    packet_header     header;\n",
    "    packet_1_payload  payload;\n",
    "} packet_1_t;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80574457",
   "metadata": {},
   "source": [
    "#### 2.Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380794c0",
   "metadata": {},
   "source": [
    "\n",
    "Threading: In ai_robot_link, two communication threads are defined, one for data transmission and the other for data reception.<br>\n",
    ".rx_task() is the target function of the receiving thread, responsible for receiving the current position information sent by the friendly robot.\n",
    ".tx_task() is the target function of the sending thread, responsible for sending the current position information of this robot to the friendly robot.\n",
    "\n",
    "Detailed functionality will be explained in further analysis during program runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d83c9",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "static int    rx_task( void *arg );\n",
    "static int    tx_task( void *arg );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e92d6",
   "metadata": {},
   "source": [
    "\n",
    "Transmission Functions: Several demonstration functions are provided in ai_robot_link, which simply transmit or receive undefined (x, y, heading) values to show the method of data transmission in a dual-robot interaction state. These functions are intended to be modified based on the actual interaction data types during competition planning and design. A common feature of these three functions is that they access and modify the data structures used by the threads:\n",
    "packet_1_t packet_tx_1;  \n",
    "packet_1_t packet_rx_1;\n",
    "To ensure thread safety and avoid data races or other concurrency issues, the use of mutexes (short for \"mutual exclusion\") is implemented:\n",
    "\n",
    "vex::mutex rxlock;  \n",
    "vex::mutex txlock;\n",
    "A mutex is a commonly used synchronization method in programming to protect access to shared resources in a multithreaded or multiprocess environment. When a thread or process acquires a mutex, other threads or processes attempting to acquire the same mutex will be blocked until the mutex is released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f606e7c",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "void set_remote_location( float x, float y, float heading, int32_t status );\n",
    "void get_local_location( float &x, float &y, float &heading, int32_t &status );\n",
    "void get_remote_location( float &x, float &y, float &heading );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558953f9",
   "metadata": {},
   "source": [
    "\n",
    "The information parsing function: Parses the transmitted position data from the received data packet.\n",
    "\n",
    "The .robot_link::process(uint8_t data) function, within the rx_task thread, sequentially reads the header, type, and checksum; performs a checksum calculation, and if it is correct, proceeds to read the transmitted data.\n",
    "\n",
    "Similar to the parse() function in the thread that reads Jetson information, the program first acquires an exclusive mutex lock, sequentially reads the synchronization code and checksum, and upon successful checksum verification, recognizes the transmitted data as correct, copies the transmitted data, and releases the mutex lock.<br>\n",
    "The following code snippet demonstrates the process of copying the valid data from the wireless port to the program variable packet_rx_1.payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83a225",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "case comms_state::kStateGoodPacket:\n",
    "    if( payload_type == RL_LOCATION_PACKET ) {\n",
    "    // lock access and copy data\n",
    "        rxlock.lock();\n",
    "        memcpy( &packet_rx_1.payload, &payload.pak_1, sizeof(packet_1_payload));\n",
    "        rxlock.unlock();\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fd227",
   "metadata": {},
   "source": [
    "###   9.&nbsp; &nbsp;Vex5 Robot Brian Side ai_functions.cpp Program Description<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae2a75",
   "metadata": {},
   "source": [
    "\n",
    "The ai_functions are the control programs for the robot's movement and gripper actions. Instead of using gamepad controls or preset movement parameters, the implementation of movement relies on real-time calculations based on the target position coordinates transmitted by the Jetson. This is the AI-driven servo-following control module.<br>\n",
    "\n",
    "Within the ai_functions section, the core function is DETECTION_OBJECT findTarget(int type), where the parameter type represents the classification of the target. The JetsonExample/labels.txt file defines the classification of three-colored balls as follows:<br>\n",
    "\n",
    "type(GreenTriball) = 0<br>\n",
    "type(RedTriball) = 1<br>\n",
    "type(BlueTriball) = 2<br>\n",
    "The main program, main(), first calls the getObject() function. The getObject() function initially directly uses findTarget(int type). If no target is found, it drives the robot to rotate and change the field of view of the robot's vision camera to re-extract the target data. Once a target is detected, it locates the gripper and drives the robot to the target's grasping position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dc243",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "void getObject(){\n",
    "    DETECTION_OBJECT target = findTarget(0);\n",
    "    // If no target found, turn and try to find again\n",
    "    if (target.mapLocation.x == 0 && target.mapLocation.y == 0){\n",
    "        Drivetrain.turnFor(45, rotationUnits::deg, 50, velocityUnits::pct);\n",
    "        target = findTarget(0);\n",
    "    }\n",
    "    double rot = Arm.position(rotationUnits::deg);\n",
    "    intake(rot-1200, 1205);\n",
    "    // Move to the detected target's position\n",
    "    moveToPosition(target.mapLocation.x*100, target.mapLocation.y*100);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16662dad",
   "metadata": {},
   "source": [
    "\n",
    "The findTarget(int type) function block is outlined as follows. It creates a space to store the data received from the Jetson and the location of the target object. It utilizes jetson_comms.get_data(&local_map) to copy the data received by the thread receive_task, which has been explained in the introduction to threading. Then, based on the type of the target object (the color of the three-colored ball), it iterates through all recognized objects to find the closest target that meets the given criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef950a",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "DETECTION_OBJECT findTarget(int type){\n",
    "    DETECTION_OBJECT target;\n",
    "    static AI_RECORD local_map;\n",
    "    jetson_comms.get_data(&local_map);\n",
    "    double lowestDist = 1000000;\n",
    "    // Iterate through detected objects to find the closest target of the specified type\n",
    "    for(int i = 0; i < local_map.detectionCount; i++) {\n",
    "        double distance = distanceTo(local_map.detections[i].mapLocation.x, local_map.detections[i].mapLocation.y);\n",
    "        if (distance < lowestDist && local_map.detections[i].classID == type) {\n",
    "            target = local_map.detections[i];\n",
    "            lowestDist = distance;\n",
    "        }\n",
    "    }\n",
    "    return target;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8ba4b",
   "metadata": {},
   "source": [
    "The other contents within ai_functions.cpp, including the motion control functions that utilize the Vex GPS sensor, follow the VEX 5 environment and do not require detailed introduction here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f871f0",
   "metadata": {},
   "source": [
    "###   10.&nbsp; &nbsp;Vex5 robot Brian end doshboard.cpp program description<br>\n",
    "The \"doshboard.cpp\" program on the Brian end of the Vex5 robot displays the execution status of Jetson and root_link on Brian's screen. This program is designed as a single-threaded function called at the beginning of the main program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf3500",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "  // start the status update display\n",
    "  thread t1(dashboardTask);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d284c",
   "metadata": {},
   "source": [
    "The program starts two threads which divide Brian's screen into two sections, each displaying the execution status of Jetson and root_link respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667ee91",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "int\n",
    "dashboardTask() {\n",
    "  while(true) {\n",
    "    // status\n",
    "    dashboardJetson(    0, 0, 280, 240 );\n",
    "    dashboardVexlink( 279, 0, 201, 240 );\n",
    "    // draw, at 30Hz\n",
    "    Brain.Screen.render();\n",
    "    this_thread::sleep_for(16);\n",
    "  }\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964be1e1",
   "metadata": {},
   "source": [
    "\n",
    "Clearly, \"dashboardJetson(0, 0, 280, 240)\" displays the communication status with Jetson, the number of received data packets, as well as the positions, classifications, and confidence levels of recognized objects (up to a maximum of 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88182756",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "Brain.Screen.printAt( ox + 10, oy += 15, \"Packets   %d\", jetson_comms.get_packets() );\n",
    "Brain.Screen.printAt( ox + 10, oy += 15, \"Errors    %d\", jetson_comms.get_errors() );\n",
    "Brain.Screen.printAt( ox + 10, oy += 15, \"Timeouts  %d\", jetson_comms.get_timeouts() );\n",
    "Brain.Screen.printAt( ox + 10, oy += 15, \"data/sec  %d             \", total_data );\n",
    "Brain.Screen.printAt( ox + 10, oy += 15, \"pkts/sec  %d             \", total_packets );\n",
    "Brain.Screen.printAt( ox + 10, oy += 15, \"count     %d\", local_map.detectionCount );\n",
    "oy += 15; // Skip a line\n",
    "\n",
    "  // once per second, update data rate stats\n",
    "if( Brain.Timer.system() > update_time ) {\n",
    "    update_time = Brain.Timer.system() + 1000;\n",
    "    total_data = jetson_comms.get_total() - last_data;\n",
    "    total_packets = jetson_comms.get_packets() - last_packets;\n",
    "    last_data = jetson_comms.get_total();\n",
    "    last_packets = jetson_comms.get_packets();\n",
    "  }\n",
    "\n",
    "  Brain.Screen.setFont( mono12 );\n",
    "  for(int i=0;i<8;i++ ) {\n",
    "    if( i < local_map.detectionCount ) {\n",
    "      Brain.Screen.printAt( ox + 10, oy += 12, \"map %d: p:%.2f c:%4d X:%.2f Y:%.2f Z:%.1f\",i,\n",
    "                           local_map.detections[i].probability,\n",
    "                           local_map.detections[i].classID,\n",
    "                           (local_map.detections[i].mapLocation.x /*/ -25.4*/),  // mm -> inches\n",
    "                           (local_map.detections[i].mapLocation.y /*/ -25.4*/),  // mm -> inches\n",
    "                           (local_map.detections[i].mapLocation.z /*/ 25.4*/)); // mm -> inches\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa37546",
   "metadata": {},
   "source": [
    "Similarly, \"dashboardVexlink(279, 0, 201, 240)\" displays the communication status of the link and the coordinates, positions, and orientations (yaw angles) of the two robots on their respective fields. It follows a similar structure as the previous program described, so I won't elaborate further here.。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
